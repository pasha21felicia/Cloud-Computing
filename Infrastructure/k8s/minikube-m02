* 
* ==> Audit <==
* |------------|-----------------------------------------------|----------|------------------|---------|---------------------|---------------------|
|  Command   |                     Args                      | Profile  |       User       | Version |     Start Time      |      End Time       |
|------------|-----------------------------------------------|----------|------------------|---------|---------------------|---------------------|
| start      |                                               | minikube | parascoviadigori | v1.32.0 | 09 Jan 24 22:42 EET |                     |
| start      |                                               | minikube | parascoviadigori | v1.32.0 | 09 Jan 24 22:45 EET | 09 Jan 24 22:46 EET |
| start      | --nodes 2                                     | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 01:26 EET | 10 Jan 24 01:26 EET |
| delete     |                                               | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 01:27 EET | 10 Jan 24 01:27 EET |
| start      | --nodes 2                                     | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 01:27 EET | 10 Jan 24 01:28 EET |
| stop       |                                               | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 01:55 EET | 10 Jan 24 01:55 EET |
| start      |                                               | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 14:56 EET |                     |
| delete     |                                               | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 15:00 EET | 10 Jan 24 15:00 EET |
| start      | --nodes 3                                     | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 15:01 EET | 10 Jan 24 15:01 EET |
| docker-env |                                               | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 15:09 EET |                     |
| ip         |                                               | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 16:47 EET | 10 Jan 24 16:47 EET |
| ssh        | -- cat                                        | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 16:58 EET |                     |
|            | /etc/kubernetes/manifests/kube-apiserver.yaml |          |                  |         |                     |                     |
| ip         |                                               | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 17:54 EET | 10 Jan 24 17:54 EET |
| ip         |                                               | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 23:10 EET | 10 Jan 24 23:10 EET |
| service    | --url backend-service                         | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 23:44 EET | 10 Jan 24 23:46 EET |
| service    | --url frontend-service                        | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 23:46 EET | 10 Jan 24 23:49 EET |
| service    | --url frontend-service                        | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 23:52 EET | 10 Jan 24 23:53 EET |
| ip         |                                               | minikube | parascoviadigori | v1.32.0 | 10 Jan 24 23:56 EET | 10 Jan 24 23:56 EET |
|------------|-----------------------------------------------|----------|------------------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2024/01/10 15:01:06
Running on machine: 192
Binary: Built with gc go1.21.4 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0110 15:01:06.795802   46847 out.go:296] Setting OutFile to fd 1 ...
I0110 15:01:06.796276   46847 out.go:348] isatty.IsTerminal(1) = true
I0110 15:01:06.796278   46847 out.go:309] Setting ErrFile to fd 2...
I0110 15:01:06.796281   46847 out.go:348] isatty.IsTerminal(2) = true
I0110 15:01:06.796377   46847 root.go:338] Updating PATH: /Users/parascoviadigori/.minikube/bin
I0110 15:01:06.796672   46847 out.go:303] Setting JSON to false
I0110 15:01:06.820923   46847 start.go:128] hostinfo: {"hostname":"192.168.1.10","uptime":458597,"bootTime":1704433069,"procs":575,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"14.2.1","kernelVersion":"23.2.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"301ac1eb-e8b1-5b99-9926-b6b8c6bfe76f"}
W0110 15:01:06.821003   46847 start.go:136] gopshost.Virtualization returned error: not implemented yet
I0110 15:01:06.824968   46847 out.go:177] üòÑ  minikube v1.32.0 on Darwin 14.2.1 (arm64)
I0110 15:01:06.843094   46847 notify.go:220] Checking for updates...
I0110 15:01:06.843917   46847 driver.go:378] Setting default libvirt URI to qemu:///system
I0110 15:01:06.844109   46847 global.go:111] Querying for installed drivers using PATH=/Users/parascoviadigori/.minikube/bin:/usr/local/opt/mysql-client/bin:/opt/homebrew/bin/python3:/Users/parascoviadigori/.nvm/versions/node/v20.10.0/bin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/Users/jetbrains/.local/bin
I0110 15:01:06.892109   46847 docker.go:122] docker version: linux-24.0.7:Docker Desktop 4.26.1 (131620)
I0110 15:01:06.892222   46847 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0110 15:01:07.183768   46847 info.go:266] docker info: {ID:6171ad98-9c96-4a51-bd4f-0c6eee52b913 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:43 OomKillDisable:false NGoroutines:68 SystemTime:2024-01-10 13:01:07.169257133 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:10 KernelVersion:6.5.11-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8227811328 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f Expected:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f} RuncCommit:{ID:v1.1.10-0-g18a0cb0 Expected:v1.1.10-0-g18a0cb0} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/parascoviadigori/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.0-desktop.2] map[Name:compose Path:/Users/parascoviadigori/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.3-desktop.2] map[Name:dev Path:/Users/parascoviadigori/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/parascoviadigori/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:/Users/parascoviadigori/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:0.1] map[Name:init Path:/Users/parascoviadigori/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.10] map[Name:sbom Path:/Users/parascoviadigori/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/parascoviadigori/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/parascoviadigori/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.2.0]] Warnings:<nil>}}
I0110 15:01:07.183823   46847 global.go:122] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0110 15:01:07.183902   46847 global.go:122] podman default: true priority: 3, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I0110 15:01:07.183907   46847 global.go:122] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0110 15:01:07.183951   46847 global.go:122] hyperkit default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "hyperkit": executable file not found in $PATH Reason: Fix:Run 'brew install hyperkit' Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/hyperkit/ Version:}
I0110 15:01:07.183979   46847 global.go:122] parallels default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "prlctl": executable file not found in $PATH Reason: Fix:Install Parallels Desktop for Mac Doc:https://minikube.sigs.k8s.io/docs/drivers/parallels/ Version:}
I0110 15:01:07.184015   46847 global.go:122] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-aarch64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I0110 15:01:07.184064   46847 global.go:122] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I0110 15:01:07.184091   46847 global.go:122] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I0110 15:01:07.184096   46847 driver.go:313] not recommending "ssh" due to default: false
I0110 15:01:07.184101   46847 driver.go:348] Picked: docker
I0110 15:01:07.184103   46847 driver.go:349] Alternatives: [ssh]
I0110 15:01:07.184107   46847 driver.go:350] Rejects: [podman hyperkit parallels qemu2 virtualbox vmware]
I0110 15:01:07.189553   46847 out.go:177] ‚ú®  Automatically selected the docker driver
I0110 15:01:07.192568   46847 start.go:298] selected driver: docker
I0110 15:01:07.192750   46847 start.go:902] validating driver "docker" against <nil>
I0110 15:01:07.192756   46847 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0110 15:01:07.193063   46847 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0110 15:01:07.272781   46847 info.go:266] docker info: {ID:6171ad98-9c96-4a51-bd4f-0c6eee52b913 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:43 OomKillDisable:false NGoroutines:68 SystemTime:2024-01-10 13:01:07.260698883 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:10 KernelVersion:6.5.11-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8227811328 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f Expected:d8f198a4ed8892c764191ef7b3b06d8a2eeb5c7f} RuncCommit:{ID:v1.1.10-0-g18a0cb0 Expected:v1.1.10-0-g18a0cb0} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined name=cgroupns] ProductLicense: Warnings:[WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/Users/parascoviadigori/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.12.0-desktop.2] map[Name:compose Path:/Users/parascoviadigori/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.23.3-desktop.2] map[Name:dev Path:/Users/parascoviadigori/.docker/cli-plugins/docker-dev SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.0] map[Name:extension Path:/Users/parascoviadigori/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.21] map[Name:feedback Path:/Users/parascoviadigori/.docker/cli-plugins/docker-feedback SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:0.1] map[Name:init Path:/Users/parascoviadigori/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v0.1.0-beta.10] map[Name:sbom Path:/Users/parascoviadigori/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scan Path:/Users/parascoviadigori/.docker/cli-plugins/docker-scan SchemaVersion:0.1.0 ShortDescription:Docker Scan Vendor:Docker Inc. Version:v0.26.0] map[Name:scout Path:/Users/parascoviadigori/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.2.0]] Warnings:<nil>}}
I0110 15:01:07.272915   46847 start_flags.go:309] no existing cluster config was found, will generate one from the flags 
I0110 15:01:07.272992   46847 start_flags.go:394] Using suggested 2200MB memory alloc based on sys=16384MB, container=7846MB
I0110 15:01:07.273077   46847 start_flags.go:913] Wait components to verify : map[apiserver:true system_pods:true]
I0110 15:01:07.276556   46847 out.go:177] üìå  Using Docker Desktop driver with root privileges
I0110 15:01:07.280152   46847 cni.go:84] Creating CNI manager for ""
I0110 15:01:07.280274   46847 cni.go:136] 0 nodes found, recommending kindnet
I0110 15:01:07.280276   46847 start_flags.go:318] Found "CNI" CNI - setting NetworkPlugin=cni
I0110 15:01:07.280281   46847 start_flags.go:323] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0110 15:01:07.282085   46847 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0110 15:01:07.288721   46847 cache.go:121] Beginning downloading kic base image for docker with docker
I0110 15:01:07.291511   46847 out.go:177] üöú  Pulling base image ...
I0110 15:01:07.297560   46847 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0110 15:01:07.297587   46847 preload.go:148] Found local preload: /Users/parascoviadigori/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4
I0110 15:01:07.297589   46847 cache.go:56] Caching tarball of preloaded images
I0110 15:01:07.297590   46847 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0110 15:01:07.297659   46847 preload.go:174] Found /Users/parascoviadigori/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0110 15:01:07.297663   46847 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0110 15:01:07.298489   46847 profile.go:148] Saving config to /Users/parascoviadigori/.minikube/profiles/minikube/config.json ...
I0110 15:01:07.298521   46847 lock.go:35] WriteFile acquiring /Users/parascoviadigori/.minikube/profiles/minikube/config.json: {Name:mk9427dbe40034b631dc1d02205465d5ad02260a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 15:01:07.336265   46847 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0110 15:01:07.336325   46847 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0110 15:01:07.336481   46847 cache.go:194] Successfully downloaded all kic artifacts
I0110 15:01:07.336769   46847 start.go:365] acquiring machines lock for minikube: {Name:mk96f99af94f3eb1581e61df50bd4259ab6cb93d Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0110 15:01:07.336821   46847 start.go:369] acquired machines lock for "minikube" in 45.333¬µs
I0110 15:01:07.336836   46847 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:} &{Name: IP: Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0110 15:01:07.336883   46847 start.go:125] createHost starting for "" (driver="docker")
I0110 15:01:07.343533   46847 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I0110 15:01:07.344108   46847 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0110 15:01:07.344388   46847 client.go:168] LocalClient.Create starting
I0110 15:01:07.344726   46847 main.go:141] libmachine: Reading certificate data from /Users/parascoviadigori/.minikube/certs/ca.pem
I0110 15:01:07.344871   46847 main.go:141] libmachine: Decoding PEM data...
I0110 15:01:07.344877   46847 main.go:141] libmachine: Parsing certificate...
I0110 15:01:07.345631   46847 main.go:141] libmachine: Reading certificate data from /Users/parascoviadigori/.minikube/certs/cert.pem
I0110 15:01:07.345767   46847 main.go:141] libmachine: Decoding PEM data...
I0110 15:01:07.345770   46847 main.go:141] libmachine: Parsing certificate...
I0110 15:01:07.346251   46847 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0110 15:01:07.383341   46847 network_create.go:77] Found existing network {name:minikube subnet:0x14002189d70 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:65535}
I0110 15:01:07.383377   46847 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0110 15:01:07.383478   46847 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0110 15:01:07.420598   46847 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0110 15:01:07.458924   46847 oci.go:103] Successfully created a docker volume minikube
I0110 15:01:07.459025   46847 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -d /var/lib
I0110 15:01:07.818077   46847 oci.go:107] Successfully prepared a docker volume minikube
I0110 15:01:07.818134   46847 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0110 15:01:07.818153   46847 kic.go:194] Starting extracting preloaded images to volume ...
I0110 15:01:07.818567   46847 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/parascoviadigori/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -I lz4 -xf /preloaded.tar -C /extractDir
I0110 15:01:09.922342   46847 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/parascoviadigori/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -I lz4 -xf /preloaded.tar -C /extractDir: (2.103710208s)
I0110 15:01:09.922362   46847 kic.go:203] duration metric: took 2.104198 seconds to extract preloaded images to volume
I0110 15:01:09.922467   46847 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0110 15:01:10.010340   46847 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0
I0110 15:01:10.186704   46847 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0110 15:01:10.227443   46847 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0110 15:01:10.266609   46847 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0110 15:01:10.364616   46847 oci.go:144] the created container "minikube" has a running status.
I0110 15:01:10.364864   46847 kic.go:225] Creating ssh key for kic: /Users/parascoviadigori/.minikube/machines/minikube/id_rsa...
I0110 15:01:10.464938   46847 kic_runner.go:191] docker (temp): /Users/parascoviadigori/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0110 15:01:10.508694   46847 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0110 15:01:10.546775   46847 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0110 15:01:10.546791   46847 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0110 15:01:10.609633   46847 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0110 15:01:10.644691   46847 machine.go:88] provisioning docker machine ...
I0110 15:01:10.644723   46847 ubuntu.go:169] provisioning hostname "minikube"
I0110 15:01:10.646843   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 15:01:10.681561   46847 main.go:141] libmachine: Using SSH client type: native
I0110 15:01:10.681870   46847 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cd2f80] 0x104cd56f0 <nil>  [] 0s} 127.0.0.1 63204 <nil> <nil>}
I0110 15:01:10.681874   46847 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0110 15:01:10.784046   46847 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0110 15:01:10.784124   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 15:01:10.822548   46847 main.go:141] libmachine: Using SSH client type: native
I0110 15:01:10.822804   46847 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cd2f80] 0x104cd56f0 <nil>  [] 0s} 127.0.0.1 63204 <nil> <nil>}
I0110 15:01:10.822812   46847 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0110 15:01:10.924528   46847 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0110 15:01:10.924539   46847 ubuntu.go:175] set auth options {CertDir:/Users/parascoviadigori/.minikube CaCertPath:/Users/parascoviadigori/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/parascoviadigori/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/parascoviadigori/.minikube/machines/server.pem ServerKeyPath:/Users/parascoviadigori/.minikube/machines/server-key.pem ClientKeyPath:/Users/parascoviadigori/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/parascoviadigori/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/parascoviadigori/.minikube}
I0110 15:01:10.924551   46847 ubuntu.go:177] setting up certificates
I0110 15:01:10.924555   46847 provision.go:83] configureAuth start
I0110 15:01:10.924637   46847 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0110 15:01:10.961691   46847 provision.go:138] copyHostCerts
I0110 15:01:10.961962   46847 exec_runner.go:144] found /Users/parascoviadigori/.minikube/ca.pem, removing ...
I0110 15:01:10.961970   46847 exec_runner.go:203] rm: /Users/parascoviadigori/.minikube/ca.pem
I0110 15:01:10.962073   46847 exec_runner.go:151] cp: /Users/parascoviadigori/.minikube/certs/ca.pem --> /Users/parascoviadigori/.minikube/ca.pem (1107 bytes)
I0110 15:01:10.962759   46847 exec_runner.go:144] found /Users/parascoviadigori/.minikube/cert.pem, removing ...
I0110 15:01:10.962762   46847 exec_runner.go:203] rm: /Users/parascoviadigori/.minikube/cert.pem
I0110 15:01:10.962831   46847 exec_runner.go:151] cp: /Users/parascoviadigori/.minikube/certs/cert.pem --> /Users/parascoviadigori/.minikube/cert.pem (1147 bytes)
I0110 15:01:10.963096   46847 exec_runner.go:144] found /Users/parascoviadigori/.minikube/key.pem, removing ...
I0110 15:01:10.963099   46847 exec_runner.go:203] rm: /Users/parascoviadigori/.minikube/key.pem
I0110 15:01:10.963276   46847 exec_runner.go:151] cp: /Users/parascoviadigori/.minikube/certs/key.pem --> /Users/parascoviadigori/.minikube/key.pem (1675 bytes)
I0110 15:01:10.963425   46847 provision.go:112] generating server cert: /Users/parascoviadigori/.minikube/machines/server.pem ca-key=/Users/parascoviadigori/.minikube/certs/ca.pem private-key=/Users/parascoviadigori/.minikube/certs/ca-key.pem org=parascoviadigori.minikube san=[192.168.49.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I0110 15:01:11.032526   46847 provision.go:172] copyRemoteCerts
I0110 15:01:11.032963   46847 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0110 15:01:11.033019   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 15:01:11.065781   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63204 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube/id_rsa Username:docker}
I0110 15:01:11.144141   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1107 bytes)
I0110 15:01:11.159107   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/machines/server.pem --> /etc/docker/server.pem (1229 bytes)
I0110 15:01:11.171763   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0110 15:01:11.183262   46847 provision.go:86] duration metric: configureAuth took 258.701916ms
I0110 15:01:11.183269   46847 ubuntu.go:193] setting minikube options for container-runtime
I0110 15:01:11.184837   46847 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0110 15:01:11.184900   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 15:01:11.224466   46847 main.go:141] libmachine: Using SSH client type: native
I0110 15:01:11.224754   46847 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cd2f80] 0x104cd56f0 <nil>  [] 0s} 127.0.0.1 63204 <nil> <nil>}
I0110 15:01:11.224765   46847 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0110 15:01:11.323780   46847 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0110 15:01:11.323789   46847 ubuntu.go:71] root file system type: overlay
I0110 15:01:11.323871   46847 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0110 15:01:11.323969   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 15:01:11.367237   46847 main.go:141] libmachine: Using SSH client type: native
I0110 15:01:11.367545   46847 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cd2f80] 0x104cd56f0 <nil>  [] 0s} 127.0.0.1 63204 <nil> <nil>}
I0110 15:01:11.367583   46847 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0110 15:01:11.476096   46847 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0110 15:01:11.476197   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 15:01:11.519502   46847 main.go:141] libmachine: Using SSH client type: native
I0110 15:01:11.519836   46847 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cd2f80] 0x104cd56f0 <nil>  [] 0s} 127.0.0.1 63204 <nil> <nil>}
I0110 15:01:11.519845   46847 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0110 15:01:11.873860   46847 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-10-26 09:06:20.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-01-10 13:01:11.474053010 +0000
@@ -1,30 +1,32 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +34,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0110 15:01:11.873880   46847 machine.go:91] provisioned docker machine in 1.229162667s
I0110 15:01:11.873887   46847 client.go:171] LocalClient.Create took 4.529473625s
I0110 15:01:11.873916   46847 start.go:167] duration metric: libmachine.API.Create for "minikube" took 4.529782917s
I0110 15:01:11.873923   46847 start.go:300] post-start starting for "minikube" (driver="docker")
I0110 15:01:11.873932   46847 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0110 15:01:11.874103   46847 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0110 15:01:11.874179   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 15:01:11.934101   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63204 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube/id_rsa Username:docker}
I0110 15:01:12.008390   46847 ssh_runner.go:195] Run: cat /etc/os-release
I0110 15:01:12.010378   46847 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0110 15:01:12.010409   46847 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0110 15:01:12.010418   46847 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0110 15:01:12.010423   46847 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0110 15:01:12.010430   46847 filesync.go:126] Scanning /Users/parascoviadigori/.minikube/addons for local assets ...
I0110 15:01:12.010565   46847 filesync.go:126] Scanning /Users/parascoviadigori/.minikube/files for local assets ...
I0110 15:01:12.010633   46847 start.go:303] post-start completed in 136.704459ms
I0110 15:01:12.011357   46847 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0110 15:01:12.059433   46847 profile.go:148] Saving config to /Users/parascoviadigori/.minikube/profiles/minikube/config.json ...
I0110 15:01:12.059924   46847 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0110 15:01:12.059976   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 15:01:12.098916   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63204 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube/id_rsa Username:docker}
I0110 15:01:12.175050   46847 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0110 15:01:12.177851   46847 start.go:128] duration metric: createHost completed in 4.840939875s
I0110 15:01:12.177858   46847 start.go:83] releasing machines lock for "minikube", held for 4.84101025s
I0110 15:01:12.177926   46847 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0110 15:01:12.218622   46847 ssh_runner.go:195] Run: cat /version.json
I0110 15:01:12.218688   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 15:01:12.219980   46847 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0110 15:01:12.220334   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 15:01:12.254912   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63204 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube/id_rsa Username:docker}
I0110 15:01:12.254912   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63204 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube/id_rsa Username:docker}
I0110 15:01:12.725185   46847 ssh_runner.go:195] Run: systemctl --version
I0110 15:01:12.733253   46847 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0110 15:01:12.740000   46847 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0110 15:01:12.759752   46847 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0110 15:01:12.759885   46847 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0110 15:01:12.774182   46847 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0110 15:01:12.774193   46847 start.go:472] detecting cgroup driver to use...
I0110 15:01:12.774205   46847 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0110 15:01:12.774831   46847 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0110 15:01:12.783312   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0110 15:01:12.788149   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0110 15:01:12.792730   46847 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0110 15:01:12.792900   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0110 15:01:12.797567   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0110 15:01:12.802024   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0110 15:01:12.806173   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0110 15:01:12.810568   46847 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0110 15:01:12.814454   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0110 15:01:12.818665   46847 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0110 15:01:12.822579   46847 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0110 15:01:12.826453   46847 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 15:01:12.859988   46847 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0110 15:01:12.902014   46847 start.go:472] detecting cgroup driver to use...
I0110 15:01:12.902034   46847 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0110 15:01:12.903011   46847 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0110 15:01:12.912036   46847 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0110 15:01:12.912179   46847 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0110 15:01:12.918240   46847 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0110 15:01:12.926418   46847 ssh_runner.go:195] Run: which cri-dockerd
I0110 15:01:12.928464   46847 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0110 15:01:12.932584   46847 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0110 15:01:12.940618   46847 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0110 15:01:12.975233   46847 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0110 15:01:13.008387   46847 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0110 15:01:13.008502   46847 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0110 15:01:13.016688   46847 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 15:01:13.046715   46847 ssh_runner.go:195] Run: sudo systemctl restart docker
I0110 15:01:13.160091   46847 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0110 15:01:13.194458   46847 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0110 15:01:13.227747   46847 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0110 15:01:13.260598   46847 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 15:01:13.291457   46847 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0110 15:01:13.309821   46847 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 15:01:13.337673   46847 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0110 15:01:13.381641   46847 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0110 15:01:13.382082   46847 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0110 15:01:13.384186   46847 start.go:540] Will wait 60s for crictl version
I0110 15:01:13.384297   46847 ssh_runner.go:195] Run: which crictl
I0110 15:01:13.386173   46847 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0110 15:01:13.408332   46847 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0110 15:01:13.408532   46847 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0110 15:01:13.422721   46847 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0110 15:01:13.438387   46847 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0110 15:01:13.438602   46847 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0110 15:01:13.536762   46847 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0110 15:01:13.537090   46847 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0110 15:01:13.539070   46847 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0110 15:01:13.543952   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0110 15:01:13.579458   46847 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0110 15:01:13.579512   46847 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0110 15:01:13.588430   46847 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0110 15:01:13.588604   46847 docker.go:601] Images already preloaded, skipping extraction
I0110 15:01:13.588805   46847 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0110 15:01:13.596701   46847 docker.go:671] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.28.3
registry.k8s.io/kube-controller-manager:v1.28.3
registry.k8s.io/kube-scheduler:v1.28.3
registry.k8s.io/kube-proxy:v1.28.3
registry.k8s.io/etcd:3.5.9-0
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0110 15:01:13.596710   46847 cache_images.go:84] Images are preloaded, skipping loading
I0110 15:01:13.596906   46847 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0110 15:01:13.626641   46847 cni.go:84] Creating CNI manager for ""
I0110 15:01:13.626648   46847 cni.go:136] 1 nodes found, recommending kindnet
I0110 15:01:13.626851   46847 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0110 15:01:13.626861   46847 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0110 15:01:13.626958   46847 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0110 15:01:13.626990   46847 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0110 15:01:13.627094   46847 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0110 15:01:13.631612   46847 binaries.go:44] Found k8s binaries, skipping transfer
I0110 15:01:13.631659   46847 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0110 15:01:13.635299   46847 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I0110 15:01:13.642506   46847 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0110 15:01:13.649642   46847 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2091 bytes)
I0110 15:01:13.656963   46847 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0110 15:01:13.658709   46847 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0110 15:01:13.664117   46847 certs.go:56] Setting up /Users/parascoviadigori/.minikube/profiles/minikube for IP: 192.168.49.2
I0110 15:01:13.664252   46847 certs.go:190] acquiring lock for shared ca certs: {Name:mk6e54a7693c0764c44ae81cc3f61b6a1f9f4699 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 15:01:13.664501   46847 certs.go:199] skipping minikubeCA CA generation: /Users/parascoviadigori/.minikube/ca.key
I0110 15:01:13.664669   46847 certs.go:199] skipping proxyClientCA CA generation: /Users/parascoviadigori/.minikube/proxy-client-ca.key
I0110 15:01:13.664700   46847 certs.go:319] generating minikube-user signed cert: /Users/parascoviadigori/.minikube/profiles/minikube/client.key
I0110 15:01:13.664715   46847 crypto.go:68] Generating cert /Users/parascoviadigori/.minikube/profiles/minikube/client.crt with IP's: []
I0110 15:01:13.843584   46847 crypto.go:156] Writing cert to /Users/parascoviadigori/.minikube/profiles/minikube/client.crt ...
I0110 15:01:13.843590   46847 lock.go:35] WriteFile acquiring /Users/parascoviadigori/.minikube/profiles/minikube/client.crt: {Name:mk852a74a8ddd3fda604f519ee61446254555662 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 15:01:13.844788   46847 crypto.go:164] Writing key to /Users/parascoviadigori/.minikube/profiles/minikube/client.key ...
I0110 15:01:13.844792   46847 lock.go:35] WriteFile acquiring /Users/parascoviadigori/.minikube/profiles/minikube/client.key: {Name:mk06da8b1e87074c0d197d9129340edc6a015101 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 15:01:13.845409   46847 certs.go:319] generating minikube signed cert: /Users/parascoviadigori/.minikube/profiles/minikube/apiserver.key.dd3b5fb2
I0110 15:01:13.845416   46847 crypto.go:68] Generating cert /Users/parascoviadigori/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 with IP's: [192.168.49.2 10.96.0.1 127.0.0.1 10.0.0.1]
I0110 15:01:13.905501   46847 crypto.go:156] Writing cert to /Users/parascoviadigori/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 ...
I0110 15:01:13.905505   46847 lock.go:35] WriteFile acquiring /Users/parascoviadigori/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2: {Name:mk446110e337d295182f90abd1a71497338eee3c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 15:01:13.905723   46847 crypto.go:164] Writing key to /Users/parascoviadigori/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 ...
I0110 15:01:13.905725   46847 lock.go:35] WriteFile acquiring /Users/parascoviadigori/.minikube/profiles/minikube/apiserver.key.dd3b5fb2: {Name:mk864eb67b613f82f3709aedfbd6427f22f2c546 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 15:01:13.905841   46847 certs.go:337] copying /Users/parascoviadigori/.minikube/profiles/minikube/apiserver.crt.dd3b5fb2 -> /Users/parascoviadigori/.minikube/profiles/minikube/apiserver.crt
I0110 15:01:13.905935   46847 certs.go:341] copying /Users/parascoviadigori/.minikube/profiles/minikube/apiserver.key.dd3b5fb2 -> /Users/parascoviadigori/.minikube/profiles/minikube/apiserver.key
I0110 15:01:13.906028   46847 certs.go:319] generating aggregator signed cert: /Users/parascoviadigori/.minikube/profiles/minikube/proxy-client.key
I0110 15:01:13.906033   46847 crypto.go:68] Generating cert /Users/parascoviadigori/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0110 15:01:13.941717   46847 crypto.go:156] Writing cert to /Users/parascoviadigori/.minikube/profiles/minikube/proxy-client.crt ...
I0110 15:01:13.941720   46847 lock.go:35] WriteFile acquiring /Users/parascoviadigori/.minikube/profiles/minikube/proxy-client.crt: {Name:mkfa7ccdc63b97fe376348fe3a26ea689f8d17be Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 15:01:13.941900   46847 crypto.go:164] Writing key to /Users/parascoviadigori/.minikube/profiles/minikube/proxy-client.key ...
I0110 15:01:13.941902   46847 lock.go:35] WriteFile acquiring /Users/parascoviadigori/.minikube/profiles/minikube/proxy-client.key: {Name:mk01448d52b3127d8614345dc9312bc0e6442431 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 15:01:13.942276   46847 certs.go:437] found cert: /Users/parascoviadigori/.minikube/certs/Users/parascoviadigori/.minikube/certs/ca-key.pem (1679 bytes)
I0110 15:01:13.942299   46847 certs.go:437] found cert: /Users/parascoviadigori/.minikube/certs/Users/parascoviadigori/.minikube/certs/ca.pem (1107 bytes)
I0110 15:01:13.942313   46847 certs.go:437] found cert: /Users/parascoviadigori/.minikube/certs/Users/parascoviadigori/.minikube/certs/cert.pem (1147 bytes)
I0110 15:01:13.942327   46847 certs.go:437] found cert: /Users/parascoviadigori/.minikube/certs/Users/parascoviadigori/.minikube/certs/key.pem (1675 bytes)
I0110 15:01:13.942626   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0110 15:01:13.968693   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0110 15:01:13.983758   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0110 15:01:14.011531   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0110 15:01:14.033197   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0110 15:01:14.044255   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0110 15:01:14.054386   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0110 15:01:14.064431   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0110 15:01:14.074721   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0110 15:01:14.085434   46847 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0110 15:01:14.093241   46847 ssh_runner.go:195] Run: openssl version
I0110 15:01:14.096164   46847 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0110 15:01:14.100567   46847 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0110 15:01:14.102220   46847 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Jan  9 20:46 /usr/share/ca-certificates/minikubeCA.pem
I0110 15:01:14.102245   46847 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0110 15:01:14.105217   46847 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0110 15:01:14.109551   46847 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0110 15:01:14.111209   46847 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I0110 15:01:14.111250   46847 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0110 15:01:14.111323   46847 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0110 15:01:14.119387   46847 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0110 15:01:14.123648   46847 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0110 15:01:14.128040   46847 kubeadm.go:226] ignoring SystemVerification for kubeadm because of docker driver
I0110 15:01:14.128096   46847 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0110 15:01:14.131820   46847 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0110 15:01:14.131842   46847 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0110 15:01:14.154929   46847 kubeadm.go:322] [init] Using Kubernetes version: v1.28.3
I0110 15:01:14.154950   46847 kubeadm.go:322] [preflight] Running pre-flight checks
I0110 15:01:14.218413   46847 kubeadm.go:322] [preflight] Pulling images required for setting up a Kubernetes cluster
I0110 15:01:14.218533   46847 kubeadm.go:322] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0110 15:01:14.218666   46847 kubeadm.go:322] [preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
I0110 15:01:14.342666   46847 kubeadm.go:322] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0110 15:01:14.347593   46847 out.go:204]     ‚ñ™ Generating certificates and keys ...
I0110 15:01:14.347860   46847 kubeadm.go:322] [certs] Using existing ca certificate authority
I0110 15:01:14.348083   46847 kubeadm.go:322] [certs] Using existing apiserver certificate and key on disk
I0110 15:01:14.384629   46847 kubeadm.go:322] [certs] Generating "apiserver-kubelet-client" certificate and key
I0110 15:01:14.438638   46847 kubeadm.go:322] [certs] Generating "front-proxy-ca" certificate and key
I0110 15:01:14.560248   46847 kubeadm.go:322] [certs] Generating "front-proxy-client" certificate and key
I0110 15:01:14.598896   46847 kubeadm.go:322] [certs] Generating "etcd/ca" certificate and key
I0110 15:01:14.721641   46847 kubeadm.go:322] [certs] Generating "etcd/server" certificate and key
I0110 15:01:14.721779   46847 kubeadm.go:322] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0110 15:01:14.845519   46847 kubeadm.go:322] [certs] Generating "etcd/peer" certificate and key
I0110 15:01:14.845650   46847 kubeadm.go:322] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0110 15:01:14.952444   46847 kubeadm.go:322] [certs] Generating "etcd/healthcheck-client" certificate and key
I0110 15:01:14.997121   46847 kubeadm.go:322] [certs] Generating "apiserver-etcd-client" certificate and key
I0110 15:01:15.083532   46847 kubeadm.go:322] [certs] Generating "sa" key and public key
I0110 15:01:15.083618   46847 kubeadm.go:322] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0110 15:01:15.154805   46847 kubeadm.go:322] [kubeconfig] Writing "admin.conf" kubeconfig file
I0110 15:01:15.228470   46847 kubeadm.go:322] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0110 15:01:15.282700   46847 kubeadm.go:322] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0110 15:01:15.473438   46847 kubeadm.go:322] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0110 15:01:15.473590   46847 kubeadm.go:322] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0110 15:01:15.474863   46847 kubeadm.go:322] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0110 15:01:15.479542   46847 out.go:204]     ‚ñ™ Booting up control plane ...
I0110 15:01:15.479811   46847 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0110 15:01:15.480070   46847 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0110 15:01:15.480318   46847 kubeadm.go:322] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0110 15:01:15.481891   46847 kubeadm.go:322] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0110 15:01:15.482219   46847 kubeadm.go:322] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0110 15:01:15.482259   46847 kubeadm.go:322] [kubelet-start] Starting the kubelet
I0110 15:01:15.530833   46847 kubeadm.go:322] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
I0110 15:01:19.032524   46847 kubeadm.go:322] [apiclient] All control plane components are healthy after 3.501373 seconds
I0110 15:01:19.032657   46847 kubeadm.go:322] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0110 15:01:19.035649   46847 kubeadm.go:322] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0110 15:01:19.553355   46847 kubeadm.go:322] [upload-certs] Skipping phase. Please see --upload-certs
I0110 15:01:19.553571   46847 kubeadm.go:322] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0110 15:01:20.059852   46847 kubeadm.go:322] [bootstrap-token] Using token: df17mu.d07os5o9i1ogpgoh
I0110 15:01:20.063314   46847 out.go:204]     ‚ñ™ Configuring RBAC rules ...
I0110 15:01:20.063609   46847 kubeadm.go:322] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0110 15:01:20.064899   46847 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0110 15:01:20.068415   46847 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0110 15:01:20.069488   46847 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0110 15:01:20.071272   46847 kubeadm.go:322] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0110 15:01:20.072472   46847 kubeadm.go:322] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0110 15:01:20.077384   46847 kubeadm.go:322] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0110 15:01:20.172236   46847 kubeadm.go:322] [addons] Applied essential addon: CoreDNS
I0110 15:01:20.471840   46847 kubeadm.go:322] [addons] Applied essential addon: kube-proxy
I0110 15:01:20.472306   46847 kubeadm.go:322] 
I0110 15:01:20.472368   46847 kubeadm.go:322] Your Kubernetes control-plane has initialized successfully!
I0110 15:01:20.472372   46847 kubeadm.go:322] 
I0110 15:01:20.472471   46847 kubeadm.go:322] To start using your cluster, you need to run the following as a regular user:
I0110 15:01:20.472475   46847 kubeadm.go:322] 
I0110 15:01:20.472505   46847 kubeadm.go:322]   mkdir -p $HOME/.kube
I0110 15:01:20.472577   46847 kubeadm.go:322]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0110 15:01:20.472630   46847 kubeadm.go:322]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0110 15:01:20.472635   46847 kubeadm.go:322] 
I0110 15:01:20.472687   46847 kubeadm.go:322] Alternatively, if you are the root user, you can run:
I0110 15:01:20.472691   46847 kubeadm.go:322] 
I0110 15:01:20.472745   46847 kubeadm.go:322]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0110 15:01:20.472753   46847 kubeadm.go:322] 
I0110 15:01:20.472818   46847 kubeadm.go:322] You should now deploy a pod network to the cluster.
I0110 15:01:20.472898   46847 kubeadm.go:322] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0110 15:01:20.472978   46847 kubeadm.go:322]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0110 15:01:20.472984   46847 kubeadm.go:322] 
I0110 15:01:20.473070   46847 kubeadm.go:322] You can now join any number of control-plane nodes by copying certificate authorities
I0110 15:01:20.473166   46847 kubeadm.go:322] and service account keys on each node and then running the following as root:
I0110 15:01:20.473170   46847 kubeadm.go:322] 
I0110 15:01:20.473413   46847 kubeadm.go:322]   kubeadm join control-plane.minikube.internal:8443 --token df17mu.d07os5o9i1ogpgoh \
I0110 15:01:20.473650   46847 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:a7d8c7afe9dfd464864f60c85f85e3fa72d93bdff7c66d1e75e1141e449c7e4e \
I0110 15:01:20.473671   46847 kubeadm.go:322] 	--control-plane 
I0110 15:01:20.473675   46847 kubeadm.go:322] 
I0110 15:01:20.473766   46847 kubeadm.go:322] Then you can join any number of worker nodes by running the following on each as root:
I0110 15:01:20.473770   46847 kubeadm.go:322] 
I0110 15:01:20.473862   46847 kubeadm.go:322] kubeadm join control-plane.minikube.internal:8443 --token df17mu.d07os5o9i1ogpgoh \
I0110 15:01:20.473971   46847 kubeadm.go:322] 	--discovery-token-ca-cert-hash sha256:a7d8c7afe9dfd464864f60c85f85e3fa72d93bdff7c66d1e75e1141e449c7e4e 
I0110 15:01:20.474812   46847 kubeadm.go:322] 	[WARNING Swap]: swap is enabled; production deployments should disable swap unless testing the NodeSwap feature gate of the kubelet
I0110 15:01:20.474997   46847 kubeadm.go:322] 	[WARNING SystemVerification]: missing optional cgroups: hugetlb
I0110 15:01:20.475359   46847 kubeadm.go:322] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0110 15:01:20.475380   46847 cni.go:84] Creating CNI manager for ""
I0110 15:01:20.475391   46847 cni.go:136] 1 nodes found, recommending kindnet
I0110 15:01:20.478591   46847 out.go:177] üîó  Configuring CNI (Container Networking Interface) ...
I0110 15:01:20.482504   46847 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I0110 15:01:20.485333   46847 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.28.3/kubectl ...
I0110 15:01:20.485341   46847 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2438 bytes)
I0110 15:01:20.495379   46847 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0110 15:01:20.784375   46847 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0110 15:01:20.784698   46847 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0110 15:01:20.784753   46847 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl label nodes minikube.k8s.io/version=v1.32.0 minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d minikube.k8s.io/name=minikube minikube.k8s.io/updated_at=2024_01_10T15_01_20_0700 minikube.k8s.io/primary=true --all --overwrite --kubeconfig=/var/lib/minikube/kubeconfig
I0110 15:01:20.821698   46847 kubeadm.go:1081] duration metric: took 37.300958ms to wait for elevateKubeSystemPrivileges.
I0110 15:01:20.821717   46847 ops.go:34] apiserver oom_adj: -16
I0110 15:01:20.821726   46847 kubeadm.go:406] StartCluster complete in 6.710444041s
I0110 15:01:20.821741   46847 settings.go:142] acquiring lock: {Name:mk4b96661f790d52e6b585a50ac492e030a71b78 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 15:01:20.821866   46847 settings.go:150] Updating kubeconfig:  /Users/parascoviadigori/.kube/config
I0110 15:01:20.823264   46847 lock.go:35] WriteFile acquiring /Users/parascoviadigori/.kube/config: {Name:mkda1787dffea1cc1ab0a4394863ae37bb2b638e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 15:01:20.823842   46847 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0110 15:01:20.824198   46847 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0110 15:01:20.824193   46847 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false]
I0110 15:01:20.824543   46847 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0110 15:01:20.824546   46847 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0110 15:01:20.824561   46847 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I0110 15:01:20.824571   46847 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0110 15:01:20.824958   46847 host.go:66] Checking if "minikube" exists ...
I0110 15:01:20.825213   46847 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0110 15:01:20.826545   46847 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0110 15:01:20.853039   46847 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0110 15:01:20.853080   46847 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true}
I0110 15:01:20.855667   46847 out.go:177] üîé  Verifying Kubernetes components...
I0110 15:01:20.862088   46847 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0110 15:01:20.866399   46847 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.28.3/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0110 15:01:20.870592   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0110 15:01:20.883955   46847 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0110 15:01:20.887034   46847 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0110 15:01:20.887039   46847 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0110 15:01:20.883103   46847 addons.go:231] Setting addon default-storageclass=true in "minikube"
I0110 15:01:20.887101   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 15:01:20.887155   46847 host.go:66] Checking if "minikube" exists ...
I0110 15:01:20.888201   46847 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0110 15:01:20.921449   46847 api_server.go:52] waiting for apiserver process to appear ...
I0110 15:01:20.921519   46847 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0110 15:01:20.933470   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63204 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube/id_rsa Username:docker}
I0110 15:01:20.933565   46847 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I0110 15:01:20.933569   46847 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0110 15:01:20.933640   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 15:01:20.969501   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63204 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube/id_rsa Username:docker}
I0110 15:01:20.992085   46847 start.go:926] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0110 15:01:20.992128   46847 api_server.go:72] duration metric: took 139.033542ms to wait for apiserver process to appear ...
I0110 15:01:20.992137   46847 api_server.go:88] waiting for apiserver healthz status ...
I0110 15:01:20.992493   46847 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:63208/healthz ...
I0110 15:01:20.995622   46847 api_server.go:279] https://127.0.0.1:63208/healthz returned 200:
ok
I0110 15:01:20.996318   46847 api_server.go:141] control plane version: v1.28.3
I0110 15:01:20.996322   46847 api_server.go:131] duration metric: took 4.1785ms to wait for apiserver health ...
I0110 15:01:20.996326   46847 system_pods.go:43] waiting for kube-system pods to appear ...
I0110 15:01:20.999702   46847 system_pods.go:59] 4 kube-system pods found
I0110 15:01:20.999707   46847 system_pods.go:61] "etcd-minikube" [4ba17337-5130-4ccc-a6ec-62f1818f05a3] Pending
I0110 15:01:20.999709   46847 system_pods.go:61] "kube-apiserver-minikube" [0178af2e-8b80-4f7b-bdb4-571ac4589726] Pending
I0110 15:01:20.999711   46847 system_pods.go:61] "kube-controller-manager-minikube" [88ef9ca1-d83f-4893-a0b6-ac3ae9cabb83] Pending
I0110 15:01:20.999713   46847 system_pods.go:61] "kube-scheduler-minikube" [31957158-3b8b-4115-8a22-6d9d794f7870] Pending
I0110 15:01:20.999714   46847 system_pods.go:74] duration metric: took 3.38725ms to wait for pod list to return data ...
I0110 15:01:20.999717   46847 kubeadm.go:581] duration metric: took 146.624625ms to wait for : map[apiserver:true system_pods:true] ...
I0110 15:01:20.999722   46847 node_conditions.go:102] verifying NodePressure condition ...
I0110 15:01:21.001046   46847 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0110 15:01:21.001051   46847 node_conditions.go:123] node cpu capacity is 8
I0110 15:01:21.001059   46847 node_conditions.go:105] duration metric: took 1.335042ms to run NodePressure ...
I0110 15:01:21.001063   46847 start.go:228] waiting for startup goroutines ...
I0110 15:01:21.034704   46847 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0110 15:01:21.044427   46847 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.28.3/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0110 15:01:21.211869   46847 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0110 15:01:21.217796   46847 addons.go:502] enable addons completed in 393.600708ms: enabled=[storage-provisioner default-storageclass]
I0110 15:01:21.217835   46847 start.go:233] waiting for cluster config update ...
I0110 15:01:21.217853   46847 start.go:242] writing updated cluster config ...
I0110 15:01:21.221071   46847 out.go:177] 
I0110 15:01:21.224440   46847 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0110 15:01:21.224501   46847 profile.go:148] Saving config to /Users/parascoviadigori/.minikube/profiles/minikube/config.json ...
I0110 15:01:21.227806   46847 out.go:177] üëç  Starting worker node minikube-m02 in cluster minikube
I0110 15:01:21.233815   46847 cache.go:121] Beginning downloading kic base image for docker with docker
I0110 15:01:21.236773   46847 out.go:177] üöú  Pulling base image ...
I0110 15:01:21.242827   46847 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0110 15:01:21.242860   46847 cache.go:56] Caching tarball of preloaded images
I0110 15:01:21.242873   46847 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0110 15:01:21.243117   46847 preload.go:174] Found /Users/parascoviadigori/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0110 15:01:21.243139   46847 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0110 15:01:21.243298   46847 profile.go:148] Saving config to /Users/parascoviadigori/.minikube/profiles/minikube/config.json ...
I0110 15:01:21.298238   46847 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0110 15:01:21.298253   46847 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0110 15:01:21.298267   46847 cache.go:194] Successfully downloaded all kic artifacts
I0110 15:01:21.298290   46847 start.go:365] acquiring machines lock for minikube-m02: {Name:mkab600975330ac7b9f52a7b799e0c13df7ce937 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0110 15:01:21.298351   46847 start.go:369] acquired machines lock for "minikube-m02" in 53.208¬µs
I0110 15:01:21.298377   46847 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP: Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:} &{Name:m02 IP: Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I0110 15:01:21.298407   46847 start.go:125] createHost starting for "m02" (driver="docker")
I0110 15:01:21.301784   46847 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I0110 15:01:21.301848   46847 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0110 15:01:21.301862   46847 client.go:168] LocalClient.Create starting
I0110 15:01:21.301914   46847 main.go:141] libmachine: Reading certificate data from /Users/parascoviadigori/.minikube/certs/ca.pem
I0110 15:01:21.301936   46847 main.go:141] libmachine: Decoding PEM data...
I0110 15:01:21.301943   46847 main.go:141] libmachine: Parsing certificate...
I0110 15:01:21.301972   46847 main.go:141] libmachine: Reading certificate data from /Users/parascoviadigori/.minikube/certs/cert.pem
I0110 15:01:21.301987   46847 main.go:141] libmachine: Decoding PEM data...
I0110 15:01:21.301991   46847 main.go:141] libmachine: Parsing certificate...
I0110 15:01:21.302422   46847 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0110 15:01:21.338106   46847 network_create.go:77] Found existing network {name:minikube subnet:0x140031ac210 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:65535}
I0110 15:01:21.338131   46847 kic.go:121] calculated static IP "192.168.49.3" for the "minikube-m02" container
I0110 15:01:21.338221   46847 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0110 15:01:21.371697   46847 cli_runner.go:164] Run: docker volume create minikube-m02 --label name.minikube.sigs.k8s.io=minikube-m02 --label created_by.minikube.sigs.k8s.io=true
I0110 15:01:21.405589   46847 oci.go:103] Successfully created a docker volume minikube-m02
I0110 15:01:21.405739   46847 cli_runner.go:164] Run: docker run --rm --name minikube-m02-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --entrypoint /usr/bin/test -v minikube-m02:/var gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -d /var/lib
I0110 15:01:21.687659   46847 oci.go:107] Successfully prepared a docker volume minikube-m02
I0110 15:01:21.687708   46847 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0110 15:01:21.687724   46847 kic.go:194] Starting extracting preloaded images to volume ...
I0110 15:01:21.687933   46847 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/parascoviadigori/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -I lz4 -xf /preloaded.tar -C /extractDir
I0110 15:01:23.531497   46847 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/parascoviadigori/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -I lz4 -xf /preloaded.tar -C /extractDir: (1.843495792s)
I0110 15:01:23.531530   46847 kic.go:203] duration metric: took 1.843794 seconds to extract preloaded images to volume
I0110 15:01:23.531705   46847 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0110 15:01:24.322293   46847 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m02 --name minikube-m02 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m02 --network minikube --ip 192.168.49.3 --volume minikube-m02:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0
I0110 15:01:24.511311   46847 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Running}}
I0110 15:01:24.551498   46847 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0110 15:01:24.590903   46847 cli_runner.go:164] Run: docker exec minikube-m02 stat /var/lib/dpkg/alternatives/iptables
I0110 15:01:24.656552   46847 oci.go:144] the created container "minikube-m02" has a running status.
I0110 15:01:24.656576   46847 kic.go:225] Creating ssh key for kic: /Users/parascoviadigori/.minikube/machines/minikube-m02/id_rsa...
I0110 15:01:24.719577   46847 kic_runner.go:191] docker (temp): /Users/parascoviadigori/.minikube/machines/minikube-m02/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0110 15:01:24.763920   46847 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0110 15:01:24.804606   46847 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0110 15:01:24.804618   46847 kic_runner.go:114] Args: [docker exec --privileged minikube-m02 chown docker:docker /home/docker/.ssh/authorized_keys]
I0110 15:01:24.872127   46847 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0110 15:01:24.906649   46847 machine.go:88] provisioning docker machine ...
I0110 15:01:24.906696   46847 ubuntu.go:169] provisioning hostname "minikube-m02"
I0110 15:01:24.906793   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0110 15:01:24.941284   46847 main.go:141] libmachine: Using SSH client type: native
I0110 15:01:24.941630   46847 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cd2f80] 0x104cd56f0 <nil>  [] 0s} 127.0.0.1 63230 <nil> <nil>}
I0110 15:01:24.941635   46847 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube-m02 && echo "minikube-m02" | sudo tee /etc/hostname
I0110 15:01:25.042588   46847 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m02

I0110 15:01:25.042709   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0110 15:01:25.079419   46847 main.go:141] libmachine: Using SSH client type: native
I0110 15:01:25.079697   46847 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cd2f80] 0x104cd56f0 <nil>  [] 0s} 127.0.0.1 63230 <nil> <nil>}
I0110 15:01:25.079704   46847 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m02' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m02/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m02' | sudo tee -a /etc/hosts; 
			fi
		fi
I0110 15:01:25.175880   46847 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0110 15:01:25.175907   46847 ubuntu.go:175] set auth options {CertDir:/Users/parascoviadigori/.minikube CaCertPath:/Users/parascoviadigori/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/parascoviadigori/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/parascoviadigori/.minikube/machines/server.pem ServerKeyPath:/Users/parascoviadigori/.minikube/machines/server-key.pem ClientKeyPath:/Users/parascoviadigori/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/parascoviadigori/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/parascoviadigori/.minikube}
I0110 15:01:25.175913   46847 ubuntu.go:177] setting up certificates
I0110 15:01:25.175920   46847 provision.go:83] configureAuth start
I0110 15:01:25.176020   46847 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0110 15:01:25.212260   46847 provision.go:138] copyHostCerts
I0110 15:01:25.212315   46847 exec_runner.go:144] found /Users/parascoviadigori/.minikube/ca.pem, removing ...
I0110 15:01:25.212319   46847 exec_runner.go:203] rm: /Users/parascoviadigori/.minikube/ca.pem
I0110 15:01:25.212405   46847 exec_runner.go:151] cp: /Users/parascoviadigori/.minikube/certs/ca.pem --> /Users/parascoviadigori/.minikube/ca.pem (1107 bytes)
I0110 15:01:25.212589   46847 exec_runner.go:144] found /Users/parascoviadigori/.minikube/cert.pem, removing ...
I0110 15:01:25.212591   46847 exec_runner.go:203] rm: /Users/parascoviadigori/.minikube/cert.pem
I0110 15:01:25.212634   46847 exec_runner.go:151] cp: /Users/parascoviadigori/.minikube/certs/cert.pem --> /Users/parascoviadigori/.minikube/cert.pem (1147 bytes)
I0110 15:01:25.212755   46847 exec_runner.go:144] found /Users/parascoviadigori/.minikube/key.pem, removing ...
I0110 15:01:25.212756   46847 exec_runner.go:203] rm: /Users/parascoviadigori/.minikube/key.pem
I0110 15:01:25.212794   46847 exec_runner.go:151] cp: /Users/parascoviadigori/.minikube/certs/key.pem --> /Users/parascoviadigori/.minikube/key.pem (1675 bytes)
I0110 15:01:25.213655   46847 provision.go:112] generating server cert: /Users/parascoviadigori/.minikube/machines/server.pem ca-key=/Users/parascoviadigori/.minikube/certs/ca.pem private-key=/Users/parascoviadigori/.minikube/certs/ca-key.pem org=parascoviadigori.minikube-m02 san=[192.168.49.3 127.0.0.1 localhost 127.0.0.1 minikube minikube-m02]
I0110 15:01:25.452386   46847 provision.go:172] copyRemoteCerts
I0110 15:01:25.452459   46847 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0110 15:01:25.452494   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0110 15:01:25.487849   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63230 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0110 15:01:25.571210   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/machines/server.pem --> /etc/docker/server.pem (1241 bytes)
I0110 15:01:25.585764   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0110 15:01:25.598030   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1107 bytes)
I0110 15:01:25.609064   46847 provision.go:86] duration metric: configureAuth took 433.137667ms
I0110 15:01:25.609070   46847 ubuntu.go:193] setting minikube options for container-runtime
I0110 15:01:25.611678   46847 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0110 15:01:25.611761   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0110 15:01:25.650809   46847 main.go:141] libmachine: Using SSH client type: native
I0110 15:01:25.651088   46847 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cd2f80] 0x104cd56f0 <nil>  [] 0s} 127.0.0.1 63230 <nil> <nil>}
I0110 15:01:25.651091   46847 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0110 15:01:25.746867   46847 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0110 15:01:25.746882   46847 ubuntu.go:71] root file system type: overlay
I0110 15:01:25.746978   46847 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0110 15:01:25.747076   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0110 15:01:25.787665   46847 main.go:141] libmachine: Using SSH client type: native
I0110 15:01:25.787952   46847 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cd2f80] 0x104cd56f0 <nil>  [] 0s} 127.0.0.1 63230 <nil> <nil>}
I0110 15:01:25.787988   46847 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.49.2"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0110 15:01:25.887993   46847 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.49.2


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0110 15:01:25.888105   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0110 15:01:25.925305   46847 main.go:141] libmachine: Using SSH client type: native
I0110 15:01:25.925589   46847 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cd2f80] 0x104cd56f0 <nil>  [] 0s} 127.0.0.1 63230 <nil> <nil>}
I0110 15:01:25.925596   46847 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0110 15:01:26.263084   46847 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-10-26 09:06:20.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-01-10 13:01:25.885710003 +0000
@@ -1,30 +1,33 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Environment=NO_PROXY=192.168.49.2
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +35,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0110 15:01:26.263115   46847 machine.go:91] provisioned docker machine in 1.356432333s
I0110 15:01:26.263126   46847 client.go:171] LocalClient.Create took 4.961236541s
I0110 15:01:26.263148   46847 start.go:167] duration metric: libmachine.API.Create for "minikube" took 4.961273166s
I0110 15:01:26.263154   46847 start.go:300] post-start starting for "minikube-m02" (driver="docker")
I0110 15:01:26.263163   46847 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0110 15:01:26.263340   46847 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0110 15:01:26.263417   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0110 15:01:26.318935   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63230 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0110 15:01:26.391868   46847 ssh_runner.go:195] Run: cat /etc/os-release
I0110 15:01:26.393780   46847 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0110 15:01:26.393812   46847 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0110 15:01:26.393823   46847 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0110 15:01:26.393828   46847 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0110 15:01:26.393835   46847 filesync.go:126] Scanning /Users/parascoviadigori/.minikube/addons for local assets ...
I0110 15:01:26.393951   46847 filesync.go:126] Scanning /Users/parascoviadigori/.minikube/files for local assets ...
I0110 15:01:26.393994   46847 start.go:303] post-start completed in 130.835208ms
I0110 15:01:26.394765   46847 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0110 15:01:26.441202   46847 profile.go:148] Saving config to /Users/parascoviadigori/.minikube/profiles/minikube/config.json ...
I0110 15:01:26.441873   46847 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0110 15:01:26.441926   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0110 15:01:26.479874   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63230 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0110 15:01:26.552625   46847 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0110 15:01:26.555105   46847 start.go:128] duration metric: createHost completed in 5.256662125s
I0110 15:01:26.555121   46847 start.go:83] releasing machines lock for "minikube-m02", held for 5.256740541s
I0110 15:01:26.555196   46847 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0110 15:01:26.603313   46847 out.go:177] üåê  Found network options:
I0110 15:01:26.605814   46847 out.go:177]     ‚ñ™ NO_PROXY=192.168.49.2
W0110 15:01:26.609582   46847 proxy.go:119] fail to check proxy env: Error ip not in block
W0110 15:01:26.609602   46847 proxy.go:119] fail to check proxy env: Error ip not in block
I0110 15:01:26.609708   46847 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0110 15:01:26.609709   46847 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0110 15:01:26.609755   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0110 15:01:26.609757   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0110 15:01:26.645458   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63230 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0110 15:01:26.645458   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63230 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube-m02/id_rsa Username:docker}
I0110 15:01:27.093662   46847 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0110 15:01:27.112119   46847 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0110 15:01:27.112224   46847 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0110 15:01:27.126843   46847 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0110 15:01:27.126853   46847 start.go:472] detecting cgroup driver to use...
I0110 15:01:27.126865   46847 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0110 15:01:27.126963   46847 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0110 15:01:27.134699   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0110 15:01:27.139319   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0110 15:01:27.143708   46847 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0110 15:01:27.143786   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0110 15:01:27.148196   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0110 15:01:27.152476   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0110 15:01:27.156624   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0110 15:01:27.160752   46847 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0110 15:01:27.164741   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0110 15:01:27.169055   46847 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0110 15:01:27.172765   46847 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0110 15:01:27.176482   46847 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 15:01:27.204763   46847 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0110 15:01:27.254775   46847 start.go:472] detecting cgroup driver to use...
I0110 15:01:27.254795   46847 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0110 15:01:27.254930   46847 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0110 15:01:27.268758   46847 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0110 15:01:27.268898   46847 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0110 15:01:27.275000   46847 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0110 15:01:27.283342   46847 ssh_runner.go:195] Run: which cri-dockerd
I0110 15:01:27.285545   46847 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0110 15:01:27.289873   46847 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0110 15:01:27.299103   46847 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0110 15:01:27.334674   46847 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0110 15:01:27.367072   46847 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0110 15:01:27.367109   46847 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0110 15:01:27.376315   46847 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 15:01:27.424390   46847 ssh_runner.go:195] Run: sudo systemctl restart docker
I0110 15:01:27.527829   46847 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0110 15:01:27.562140   46847 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0110 15:01:27.595817   46847 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0110 15:01:27.628303   46847 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 15:01:27.662486   46847 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0110 15:01:27.676964   46847 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 15:01:27.709258   46847 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0110 15:01:27.753913   46847 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0110 15:01:27.754095   46847 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0110 15:01:27.756161   46847 start.go:540] Will wait 60s for crictl version
I0110 15:01:27.756270   46847 ssh_runner.go:195] Run: which crictl
I0110 15:01:27.758164   46847 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0110 15:01:27.779419   46847 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0110 15:01:27.779517   46847 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0110 15:01:27.790872   46847 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0110 15:01:27.804363   46847 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0110 15:01:27.810343   46847 out.go:177]     ‚ñ™ env NO_PROXY=192.168.49.2
I0110 15:01:27.815668   46847 cli_runner.go:164] Run: docker exec -t minikube-m02 dig +short host.docker.internal
I0110 15:01:27.910588   46847 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0110 15:01:27.910722   46847 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0110 15:01:27.913298   46847 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0110 15:01:27.918868   46847 certs.go:56] Setting up /Users/parascoviadigori/.minikube/profiles/minikube for IP: 192.168.49.3
I0110 15:01:27.918878   46847 certs.go:190] acquiring lock for shared ca certs: {Name:mk6e54a7693c0764c44ae81cc3f61b6a1f9f4699 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 15:01:27.919020   46847 certs.go:199] skipping minikubeCA CA generation: /Users/parascoviadigori/.minikube/ca.key
I0110 15:01:27.919274   46847 certs.go:199] skipping proxyClientCA CA generation: /Users/parascoviadigori/.minikube/proxy-client-ca.key
I0110 15:01:27.919354   46847 certs.go:437] found cert: /Users/parascoviadigori/.minikube/certs/Users/parascoviadigori/.minikube/certs/ca-key.pem (1679 bytes)
I0110 15:01:27.919374   46847 certs.go:437] found cert: /Users/parascoviadigori/.minikube/certs/Users/parascoviadigori/.minikube/certs/ca.pem (1107 bytes)
I0110 15:01:27.919394   46847 certs.go:437] found cert: /Users/parascoviadigori/.minikube/certs/Users/parascoviadigori/.minikube/certs/cert.pem (1147 bytes)
I0110 15:01:27.919412   46847 certs.go:437] found cert: /Users/parascoviadigori/.minikube/certs/Users/parascoviadigori/.minikube/certs/key.pem (1675 bytes)
I0110 15:01:27.919671   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0110 15:01:27.929984   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0110 15:01:27.940521   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0110 15:01:27.950622   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0110 15:01:27.960569   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0110 15:01:27.970690   46847 ssh_runner.go:195] Run: openssl version
I0110 15:01:27.973062   46847 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0110 15:01:27.977192   46847 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0110 15:01:27.979004   46847 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Jan  9 20:46 /usr/share/ca-certificates/minikubeCA.pem
I0110 15:01:27.979065   46847 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0110 15:01:27.982519   46847 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0110 15:01:27.987306   46847 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0110 15:01:27.994276   46847 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I0110 15:01:27.994496   46847 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0110 15:01:28.033160   46847 cni.go:84] Creating CNI manager for ""
I0110 15:01:28.033168   46847 cni.go:136] 2 nodes found, recommending kindnet
I0110 15:01:28.033176   46847 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0110 15:01:28.033192   46847 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.3 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube-m02 DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.3 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0110 15:01:28.033306   46847 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.3
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube-m02"
  kubeletExtraArgs:
    node-ip: 192.168.49.3
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0110 15:01:28.033365   46847 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube-m02 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.3

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0110 15:01:28.033510   46847 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0110 15:01:28.038308   46847 binaries.go:44] Found k8s binaries, skipping transfer
I0110 15:01:28.038395   46847 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0110 15:01:28.042702   46847 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (373 bytes)
I0110 15:01:28.050606   46847 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0110 15:01:28.059243   46847 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0110 15:01:28.061188   46847 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0110 15:01:28.066533   46847 host.go:66] Checking if "minikube" exists ...
I0110 15:01:28.066738   46847 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0110 15:01:28.066769   46847 start.go:304] JoinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0110 15:01:28.066829   46847 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm token create --print-join-command --ttl=0"
I0110 15:01:28.066897   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 15:01:28.119750   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63204 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube/id_rsa Username:docker}
I0110 15:01:28.215353   46847 start.go:325] trying to join worker node "m02" to cluster: &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I0110 15:01:28.215411   46847 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token jz6bi6.wryz4rtm6xxs5h5q --discovery-token-ca-cert-hash sha256:a7d8c7afe9dfd464864f60c85f85e3fa72d93bdff7c66d1e75e1141e449c7e4e --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m02"
I0110 15:01:35.226682   46847 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token jz6bi6.wryz4rtm6xxs5h5q --discovery-token-ca-cert-hash sha256:a7d8c7afe9dfd464864f60c85f85e3fa72d93bdff7c66d1e75e1141e449c7e4e --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m02": (7.011205958s)
I0110 15:01:35.226768   46847 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I0110 15:01:35.312911   46847 start.go:306] JoinCluster complete in 7.246082333s
I0110 15:01:35.312942   46847 cni.go:84] Creating CNI manager for ""
I0110 15:01:35.312950   46847 cni.go:136] 2 nodes found, recommending kindnet
I0110 15:01:35.313299   46847 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I0110 15:01:35.315592   46847 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.28.3/kubectl ...
I0110 15:01:35.315599   46847 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2438 bytes)
I0110 15:01:35.323512   46847 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0110 15:01:35.429162   46847 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0110 15:01:35.429176   46847 start.go:223] Will wait 6m0s for node &{Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I0110 15:01:35.435266   46847 out.go:177] üîé  Verifying Kubernetes components...
I0110 15:01:35.440918   46847 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0110 15:01:35.446883   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0110 15:01:35.486760   46847 kubeadm.go:581] duration metric: took 57.570166ms to wait for : map[apiserver:true system_pods:true] ...
I0110 15:01:35.486777   46847 node_conditions.go:102] verifying NodePressure condition ...
I0110 15:01:35.488731   46847 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0110 15:01:35.488738   46847 node_conditions.go:123] node cpu capacity is 8
I0110 15:01:35.488746   46847 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0110 15:01:35.488747   46847 node_conditions.go:123] node cpu capacity is 8
I0110 15:01:35.488749   46847 node_conditions.go:105] duration metric: took 1.970042ms to run NodePressure ...
I0110 15:01:35.488753   46847 start.go:228] waiting for startup goroutines ...
I0110 15:01:35.488770   46847 start.go:242] writing updated cluster config ...
I0110 15:01:35.492853   46847 out.go:177] 
I0110 15:01:35.494427   46847 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0110 15:01:35.494464   46847 profile.go:148] Saving config to /Users/parascoviadigori/.minikube/profiles/minikube/config.json ...
I0110 15:01:35.497841   46847 out.go:177] üëç  Starting worker node minikube-m03 in cluster minikube
I0110 15:01:35.503863   46847 cache.go:121] Beginning downloading kic base image for docker with docker
I0110 15:01:35.505310   46847 out.go:177] üöú  Pulling base image ...
I0110 15:01:35.511674   46847 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0110 15:01:35.511680   46847 cache.go:56] Caching tarball of preloaded images
I0110 15:01:35.511687   46847 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon
I0110 15:01:35.511747   46847 preload.go:174] Found /Users/parascoviadigori/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0110 15:01:35.511752   46847 cache.go:59] Finished verifying existence of preloaded tar for  v1.28.3 on docker
I0110 15:01:35.511806   46847 profile.go:148] Saving config to /Users/parascoviadigori/.minikube/profiles/minikube/config.json ...
I0110 15:01:35.547851   46847 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 in local docker daemon, skipping pull
I0110 15:01:35.547870   46847 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 exists in daemon, skipping load
I0110 15:01:35.547911   46847 cache.go:194] Successfully downloaded all kic artifacts
I0110 15:01:35.547934   46847 start.go:365] acquiring machines lock for minikube-m03: {Name:mk48d4a535d4492cd711bd0a8f9d7b459516ce9b Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0110 15:01:35.548087   46847 start.go:369] acquired machines lock for "minikube-m03" in 144.875¬µs
I0110 15:01:35.548107   46847 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP: Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:} &{Name:m03 IP: Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I0110 15:01:35.548164   46847 start.go:125] createHost starting for "m03" (driver="docker")
I0110 15:01:35.551872   46847 out.go:204] üî•  Creating docker container (CPUs=2, Memory=2200MB) ...
I0110 15:01:35.551945   46847 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0110 15:01:35.551952   46847 client.go:168] LocalClient.Create starting
I0110 15:01:35.552025   46847 main.go:141] libmachine: Reading certificate data from /Users/parascoviadigori/.minikube/certs/ca.pem
I0110 15:01:35.552055   46847 main.go:141] libmachine: Decoding PEM data...
I0110 15:01:35.552063   46847 main.go:141] libmachine: Parsing certificate...
I0110 15:01:35.552098   46847 main.go:141] libmachine: Reading certificate data from /Users/parascoviadigori/.minikube/certs/cert.pem
I0110 15:01:35.552112   46847 main.go:141] libmachine: Decoding PEM data...
I0110 15:01:35.552116   46847 main.go:141] libmachine: Parsing certificate...
I0110 15:01:35.555147   46847 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0110 15:01:35.590491   46847 network_create.go:77] Found existing network {name:minikube subnet:0x1400285d860 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:65535}
I0110 15:01:35.590514   46847 kic.go:121] calculated static IP "192.168.49.4" for the "minikube-m03" container
I0110 15:01:35.590612   46847 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0110 15:01:35.627463   46847 cli_runner.go:164] Run: docker volume create minikube-m03 --label name.minikube.sigs.k8s.io=minikube-m03 --label created_by.minikube.sigs.k8s.io=true
I0110 15:01:35.665223   46847 oci.go:103] Successfully created a docker volume minikube-m03
I0110 15:01:35.665448   46847 cli_runner.go:164] Run: docker run --rm --name minikube-m03-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --entrypoint /usr/bin/test -v minikube-m03:/var gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -d /var/lib
I0110 15:01:35.961380   46847 oci.go:107] Successfully prepared a docker volume minikube-m03
I0110 15:01:35.961416   46847 preload.go:132] Checking if preload exists for k8s version v1.28.3 and runtime docker
I0110 15:01:35.961440   46847 kic.go:194] Starting extracting preloaded images to volume ...
I0110 15:01:35.961636   46847 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /Users/parascoviadigori/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube-m03:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -I lz4 -xf /preloaded.tar -C /extractDir
I0110 15:01:38.243089   46847 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /Users/parascoviadigori/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.28.3-docker-overlay2-arm64.tar.lz4:/preloaded.tar:ro -v minikube-m03:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 -I lz4 -xf /preloaded.tar -C /extractDir: (2.281290875s)
I0110 15:01:38.243160   46847 kic.go:203] duration metric: took 2.281705 seconds to extract preloaded images to volume
I0110 15:01:38.243682   46847 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0110 15:01:39.074414   46847 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m03 --name minikube-m03 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m03 --network minikube --ip 192.168.49.4 --volume minikube-m03:/var --security-opt apparmor=unconfined --memory=2200mb --memory-swap=2200mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0
I0110 15:01:39.429629   46847 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Running}}
I0110 15:01:39.487063   46847 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0110 15:01:39.534197   46847 cli_runner.go:164] Run: docker exec minikube-m03 stat /var/lib/dpkg/alternatives/iptables
I0110 15:01:39.639911   46847 oci.go:144] the created container "minikube-m03" has a running status.
I0110 15:01:39.639943   46847 kic.go:225] Creating ssh key for kic: /Users/parascoviadigori/.minikube/machines/minikube-m03/id_rsa...
I0110 15:01:39.910028   46847 kic_runner.go:191] docker (temp): /Users/parascoviadigori/.minikube/machines/minikube-m03/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0110 15:01:39.965298   46847 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0110 15:01:40.010262   46847 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0110 15:01:40.010276   46847 kic_runner.go:114] Args: [docker exec --privileged minikube-m03 chown docker:docker /home/docker/.ssh/authorized_keys]
I0110 15:01:40.094438   46847 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0110 15:01:40.139785   46847 machine.go:88] provisioning docker machine ...
I0110 15:01:40.139822   46847 ubuntu.go:169] provisioning hostname "minikube-m03"
I0110 15:01:40.139965   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0110 15:01:40.177170   46847 main.go:141] libmachine: Using SSH client type: native
I0110 15:01:40.177551   46847 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cd2f80] 0x104cd56f0 <nil>  [] 0s} 127.0.0.1 63286 <nil> <nil>}
I0110 15:01:40.177556   46847 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube-m03 && echo "minikube-m03" | sudo tee /etc/hostname
I0110 15:01:40.287110   46847 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m03

I0110 15:01:40.287232   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0110 15:01:40.334115   46847 main.go:141] libmachine: Using SSH client type: native
I0110 15:01:40.334660   46847 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cd2f80] 0x104cd56f0 <nil>  [] 0s} 127.0.0.1 63286 <nil> <nil>}
I0110 15:01:40.334677   46847 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m03' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m03/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m03' | sudo tee -a /etc/hosts; 
			fi
		fi
I0110 15:01:40.434059   46847 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0110 15:01:40.434072   46847 ubuntu.go:175] set auth options {CertDir:/Users/parascoviadigori/.minikube CaCertPath:/Users/parascoviadigori/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/parascoviadigori/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/parascoviadigori/.minikube/machines/server.pem ServerKeyPath:/Users/parascoviadigori/.minikube/machines/server-key.pem ClientKeyPath:/Users/parascoviadigori/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/parascoviadigori/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/parascoviadigori/.minikube}
I0110 15:01:40.434078   46847 ubuntu.go:177] setting up certificates
I0110 15:01:40.434081   46847 provision.go:83] configureAuth start
I0110 15:01:40.434161   46847 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0110 15:01:40.473032   46847 provision.go:138] copyHostCerts
I0110 15:01:40.473099   46847 exec_runner.go:144] found /Users/parascoviadigori/.minikube/ca.pem, removing ...
I0110 15:01:40.473103   46847 exec_runner.go:203] rm: /Users/parascoviadigori/.minikube/ca.pem
I0110 15:01:40.473197   46847 exec_runner.go:151] cp: /Users/parascoviadigori/.minikube/certs/ca.pem --> /Users/parascoviadigori/.minikube/ca.pem (1107 bytes)
I0110 15:01:40.475408   46847 exec_runner.go:144] found /Users/parascoviadigori/.minikube/cert.pem, removing ...
I0110 15:01:40.475411   46847 exec_runner.go:203] rm: /Users/parascoviadigori/.minikube/cert.pem
I0110 15:01:40.475492   46847 exec_runner.go:151] cp: /Users/parascoviadigori/.minikube/certs/cert.pem --> /Users/parascoviadigori/.minikube/cert.pem (1147 bytes)
I0110 15:01:40.475608   46847 exec_runner.go:144] found /Users/parascoviadigori/.minikube/key.pem, removing ...
I0110 15:01:40.475609   46847 exec_runner.go:203] rm: /Users/parascoviadigori/.minikube/key.pem
I0110 15:01:40.475647   46847 exec_runner.go:151] cp: /Users/parascoviadigori/.minikube/certs/key.pem --> /Users/parascoviadigori/.minikube/key.pem (1675 bytes)
I0110 15:01:40.476069   46847 provision.go:112] generating server cert: /Users/parascoviadigori/.minikube/machines/server.pem ca-key=/Users/parascoviadigori/.minikube/certs/ca.pem private-key=/Users/parascoviadigori/.minikube/certs/ca-key.pem org=parascoviadigori.minikube-m03 san=[192.168.49.4 127.0.0.1 localhost 127.0.0.1 minikube minikube-m03]
I0110 15:01:40.829171   46847 provision.go:172] copyRemoteCerts
I0110 15:01:40.829250   46847 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0110 15:01:40.829288   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0110 15:01:40.872916   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63286 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube-m03/id_rsa Username:docker}
I0110 15:01:40.946698   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1107 bytes)
I0110 15:01:40.960958   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/machines/server.pem --> /etc/docker/server.pem (1237 bytes)
I0110 15:01:40.989134   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0110 15:01:41.000428   46847 provision.go:86] duration metric: configureAuth took 566.339ms
I0110 15:01:41.000435   46847 ubuntu.go:193] setting minikube options for container-runtime
I0110 15:01:41.001240   46847 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0110 15:01:41.001315   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0110 15:01:41.045322   46847 main.go:141] libmachine: Using SSH client type: native
I0110 15:01:41.045606   46847 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cd2f80] 0x104cd56f0 <nil>  [] 0s} 127.0.0.1 63286 <nil> <nil>}
I0110 15:01:41.045609   46847 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0110 15:01:41.143828   46847 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0110 15:01:41.143834   46847 ubuntu.go:71] root file system type: overlay
I0110 15:01:41.143911   46847 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0110 15:01:41.143998   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0110 15:01:41.181969   46847 main.go:141] libmachine: Using SSH client type: native
I0110 15:01:41.182229   46847 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cd2f80] 0x104cd56f0 <nil>  [] 0s} 127.0.0.1 63286 <nil> <nil>}
I0110 15:01:41.182262   46847 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.49.2"
Environment="NO_PROXY=192.168.49.2,192.168.49.3"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0110 15:01:41.285496   46847 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.49.2
Environment=NO_PROXY=192.168.49.2,192.168.49.3


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0110 15:01:41.285679   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0110 15:01:41.328797   46847 main.go:141] libmachine: Using SSH client type: native
I0110 15:01:41.329088   46847 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x104cd2f80] 0x104cd56f0 <nil>  [] 0s} 127.0.0.1 63286 <nil> <nil>}
I0110 15:01:41.329095   46847 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0110 15:01:41.677909   46847 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2023-10-26 09:06:20.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2024-01-10 13:01:41.283710010 +0000
@@ -1,30 +1,34 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Environment=NO_PROXY=192.168.49.2
+Environment=NO_PROXY=192.168.49.2,192.168.49.3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
@@ -32,16 +36,16 @@
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0110 15:01:41.677923   46847 machine.go:91] provisioned docker machine in 1.5381095s
I0110 15:01:41.677931   46847 client.go:171] LocalClient.Create took 6.12594575s
I0110 15:01:41.677955   46847 start.go:167] duration metric: libmachine.API.Create for "minikube" took 6.125978542s
I0110 15:01:41.677961   46847 start.go:300] post-start starting for "minikube-m03" (driver="docker")
I0110 15:01:41.677969   46847 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0110 15:01:41.678133   46847 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0110 15:01:41.678205   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0110 15:01:41.730462   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63286 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube-m03/id_rsa Username:docker}
I0110 15:01:41.804507   46847 ssh_runner.go:195] Run: cat /etc/os-release
I0110 15:01:41.806376   46847 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0110 15:01:41.806403   46847 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0110 15:01:41.806412   46847 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0110 15:01:41.806416   46847 info.go:137] Remote host: Ubuntu 22.04.3 LTS
I0110 15:01:41.806423   46847 filesync.go:126] Scanning /Users/parascoviadigori/.minikube/addons for local assets ...
I0110 15:01:41.806754   46847 filesync.go:126] Scanning /Users/parascoviadigori/.minikube/files for local assets ...
I0110 15:01:41.806818   46847 start.go:303] post-start completed in 128.851959ms
I0110 15:01:41.807683   46847 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0110 15:01:41.849853   46847 profile.go:148] Saving config to /Users/parascoviadigori/.minikube/profiles/minikube/config.json ...
I0110 15:01:41.850316   46847 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0110 15:01:41.850362   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0110 15:01:41.886350   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63286 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube-m03/id_rsa Username:docker}
I0110 15:01:41.958290   46847 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0110 15:01:41.960523   46847 start.go:128] duration metric: createHost completed in 6.412320542s
I0110 15:01:41.960530   46847 start.go:83] releasing machines lock for "minikube-m03", held for 6.412408708s
I0110 15:01:41.960605   46847 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0110 15:01:42.005184   46847 out.go:177] üåê  Found network options:
I0110 15:01:42.007219   46847 out.go:177]     ‚ñ™ NO_PROXY=192.168.49.2,192.168.49.3
W0110 15:01:42.014808   46847 proxy.go:119] fail to check proxy env: Error ip not in block
W0110 15:01:42.014819   46847 proxy.go:119] fail to check proxy env: Error ip not in block
W0110 15:01:42.014830   46847 proxy.go:119] fail to check proxy env: Error ip not in block
W0110 15:01:42.014834   46847 proxy.go:119] fail to check proxy env: Error ip not in block
I0110 15:01:42.015000   46847 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0110 15:01:42.015053   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0110 15:01:42.015976   46847 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0110 15:01:42.016112   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0110 15:01:42.121100   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63286 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube-m03/id_rsa Username:docker}
I0110 15:01:42.121393   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63286 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube-m03/id_rsa Username:docker}
I0110 15:01:42.381945   46847 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0110 15:01:42.395067   46847 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0110 15:01:42.395212   46847 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0110 15:01:42.407985   46847 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0110 15:01:42.407998   46847 start.go:472] detecting cgroup driver to use...
I0110 15:01:42.408012   46847 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0110 15:01:42.408126   46847 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0110 15:01:42.415232   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0110 15:01:42.419934   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0110 15:01:42.424992   46847 containerd.go:145] configuring containerd to use "cgroupfs" as cgroup driver...
I0110 15:01:42.425068   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0110 15:01:42.430261   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0110 15:01:42.434934   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0110 15:01:42.439424   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0110 15:01:42.443934   46847 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0110 15:01:42.449048   46847 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0110 15:01:42.453636   46847 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0110 15:01:42.457595   46847 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0110 15:01:42.461747   46847 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 15:01:42.496746   46847 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0110 15:01:42.536286   46847 start.go:472] detecting cgroup driver to use...
I0110 15:01:42.536304   46847 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0110 15:01:42.536454   46847 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0110 15:01:42.545595   46847 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0110 15:01:42.545712   46847 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0110 15:01:42.551662   46847 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0110 15:01:42.562931   46847 ssh_runner.go:195] Run: which cri-dockerd
I0110 15:01:42.566115   46847 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0110 15:01:42.572830   46847 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0110 15:01:42.582866   46847 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0110 15:01:42.619572   46847 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0110 15:01:42.655934   46847 docker.go:560] configuring docker to use "cgroupfs" as cgroup driver...
I0110 15:01:42.655974   46847 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0110 15:01:42.665094   46847 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 15:01:42.700172   46847 ssh_runner.go:195] Run: sudo systemctl restart docker
I0110 15:01:42.810377   46847 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0110 15:01:42.846907   46847 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0110 15:01:42.878163   46847 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0110 15:01:42.909756   46847 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 15:01:42.945212   46847 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0110 15:01:42.964002   46847 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0110 15:01:42.999925   46847 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I0110 15:01:43.049194   46847 start.go:519] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0110 15:01:43.049386   46847 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0110 15:01:43.051900   46847 start.go:540] Will wait 60s for crictl version
I0110 15:01:43.051984   46847 ssh_runner.go:195] Run: which crictl
I0110 15:01:43.053903   46847 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0110 15:01:43.083261   46847 start.go:556] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.7
RuntimeApiVersion:  v1
I0110 15:01:43.083372   46847 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0110 15:01:43.102125   46847 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0110 15:01:43.117189   46847 out.go:204] üê≥  Preparing Kubernetes v1.28.3 on Docker 24.0.7 ...
I0110 15:01:43.123126   46847 out.go:177]     ‚ñ™ env NO_PROXY=192.168.49.2
I0110 15:01:43.126206   46847 out.go:177]     ‚ñ™ env NO_PROXY=192.168.49.2,192.168.49.3
I0110 15:01:43.129338   46847 cli_runner.go:164] Run: docker exec -t minikube-m03 dig +short host.docker.internal
I0110 15:01:43.229320   46847 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0110 15:01:43.231426   46847 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0110 15:01:43.233833   46847 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0110 15:01:43.239902   46847 certs.go:56] Setting up /Users/parascoviadigori/.minikube/profiles/minikube for IP: 192.168.49.4
I0110 15:01:43.239910   46847 certs.go:190] acquiring lock for shared ca certs: {Name:mk6e54a7693c0764c44ae81cc3f61b6a1f9f4699 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0110 15:01:43.240035   46847 certs.go:199] skipping minikubeCA CA generation: /Users/parascoviadigori/.minikube/ca.key
I0110 15:01:43.240234   46847 certs.go:199] skipping proxyClientCA CA generation: /Users/parascoviadigori/.minikube/proxy-client-ca.key
I0110 15:01:43.240343   46847 certs.go:437] found cert: /Users/parascoviadigori/.minikube/certs/Users/parascoviadigori/.minikube/certs/ca-key.pem (1679 bytes)
I0110 15:01:43.240369   46847 certs.go:437] found cert: /Users/parascoviadigori/.minikube/certs/Users/parascoviadigori/.minikube/certs/ca.pem (1107 bytes)
I0110 15:01:43.240391   46847 certs.go:437] found cert: /Users/parascoviadigori/.minikube/certs/Users/parascoviadigori/.minikube/certs/cert.pem (1147 bytes)
I0110 15:01:43.240406   46847 certs.go:437] found cert: /Users/parascoviadigori/.minikube/certs/Users/parascoviadigori/.minikube/certs/key.pem (1675 bytes)
I0110 15:01:43.240653   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0110 15:01:43.251364   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0110 15:01:43.262351   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0110 15:01:43.273220   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0110 15:01:43.284127   46847 ssh_runner.go:362] scp /Users/parascoviadigori/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0110 15:01:43.294934   46847 ssh_runner.go:195] Run: openssl version
I0110 15:01:43.298148   46847 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0110 15:01:43.303162   46847 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0110 15:01:43.305255   46847 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Jan  9 20:46 /usr/share/ca-certificates/minikubeCA.pem
I0110 15:01:43.305321   46847 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0110 15:01:43.308905   46847 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0110 15:01:43.313622   46847 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I0110 15:01:43.315362   46847 certs.go:353] certs directory doesn't exist, likely first start: ls /var/lib/minikube/certs/etcd: Process exited with status 2
stdout:

stderr:
ls: cannot access '/var/lib/minikube/certs/etcd': No such file or directory
I0110 15:01:43.315493   46847 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0110 15:01:43.345731   46847 cni.go:84] Creating CNI manager for ""
I0110 15:01:43.345739   46847 cni.go:136] 3 nodes found, recommending kindnet
I0110 15:01:43.345747   46847 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0110 15:01:43.345770   46847 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.4 APIServerPort:8443 KubernetesVersion:v1.28.3 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube-m03 DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.4 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0110 15:01:43.345877   46847 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.4
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube-m03"
  kubeletExtraArgs:
    node-ip: 192.168.49.4
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.28.3
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0110 15:01:43.345920   46847 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.28.3/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube-m03 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.4

[Install]
 config:
{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0110 15:01:43.346061   46847 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.28.3
I0110 15:01:43.350478   46847 binaries.go:44] Found k8s binaries, skipping transfer
I0110 15:01:43.350568   46847 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0110 15:01:43.354790   46847 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (373 bytes)
I0110 15:01:43.362745   46847 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0110 15:01:43.370684   46847 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0110 15:01:43.372654   46847 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0110 15:01:43.377743   46847 host.go:66] Checking if "minikube" exists ...
I0110 15:01:43.378035   46847 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.28.3
I0110 15:01:43.380047   46847 start.go:304] JoinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.42@sha256:d35ac07dfda971cabee05e0deca8aeac772f885a5348e1a0c0b0a36db20fcfc0 Memory:2200 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.28.3 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 AutoPauseInterval:1m0s GPUs:}
I0110 15:01:43.380139   46847 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm token create --print-join-command --ttl=0"
I0110 15:01:43.380217   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0110 15:01:43.431074   46847 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:63204 SSHKeyPath:/Users/parascoviadigori/.minikube/machines/minikube/id_rsa Username:docker}
I0110 15:01:43.543314   46847 start.go:325] trying to join worker node "m03" to cluster: &{Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I0110 15:01:43.543341   46847 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 6z6e6e.zey9ez4amts8cicw --discovery-token-ca-cert-hash sha256:a7d8c7afe9dfd464864f60c85f85e3fa72d93bdff7c66d1e75e1141e449c7e4e --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m03"
I0110 15:01:45.234970   46847 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.28.3:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 6z6e6e.zey9ez4amts8cicw --discovery-token-ca-cert-hash sha256:a7d8c7afe9dfd464864f60c85f85e3fa72d93bdff7c66d1e75e1141e449c7e4e --ignore-preflight-errors=all --cri-socket /var/run/cri-dockerd.sock --node-name=minikube-m03": (1.691603208s)
I0110 15:01:45.234985   46847 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I0110 15:01:45.316537   46847 start.go:306] JoinCluster complete in 1.936475417s
I0110 15:01:45.316557   46847 cni.go:84] Creating CNI manager for ""
I0110 15:01:45.316562   46847 cni.go:136] 3 nodes found, recommending kindnet
I0110 15:01:45.317420   46847 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I0110 15:01:45.320430   46847 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.28.3/kubectl ...
I0110 15:01:45.320437   46847 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2438 bytes)
I0110 15:01:45.330636   46847 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.28.3/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0110 15:01:45.514892   46847 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0110 15:01:45.514919   46847 start.go:223] Will wait 6m0s for node &{Name:m03 IP:192.168.49.4 Port:0 KubernetesVersion:v1.28.3 ContainerRuntime:docker ControlPlane:false Worker:true}
I0110 15:01:45.519666   46847 out.go:177] üîé  Verifying Kubernetes components...
I0110 15:01:45.525868   46847 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0110 15:01:45.533302   46847 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0110 15:01:45.584702   46847 kubeadm.go:581] duration metric: took 69.760042ms to wait for : map[apiserver:true system_pods:true] ...
I0110 15:01:45.584719   46847 node_conditions.go:102] verifying NodePressure condition ...
I0110 15:01:45.587005   46847 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0110 15:01:45.587009   46847 node_conditions.go:123] node cpu capacity is 8
I0110 15:01:45.587022   46847 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0110 15:01:45.587024   46847 node_conditions.go:123] node cpu capacity is 8
I0110 15:01:45.587025   46847 node_conditions.go:122] node storage ephemeral capacity is 61202244Ki
I0110 15:01:45.587026   46847 node_conditions.go:123] node cpu capacity is 8
I0110 15:01:45.587028   46847 node_conditions.go:105] duration metric: took 2.307375ms to run NodePressure ...
I0110 15:01:45.587032   46847 start.go:228] waiting for startup goroutines ...
I0110 15:01:45.587043   46847 start.go:242] writing updated cluster config ...
I0110 15:01:45.587761   46847 ssh_runner.go:195] Run: rm -f paused
I0110 15:01:45.767894   46847 start.go:600] kubectl: 1.29.0, cluster: 1.28.3 (minor skew: 1)
I0110 15:01:45.771695   46847 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Jan 10 13:01:13 minikube systemd[1]: Stopped Docker Application Container Engine.
Jan 10 13:01:13 minikube systemd[1]: Starting Docker Application Container Engine...
Jan 10 13:01:13 minikube dockerd[1039]: time="2024-01-10T13:01:13.089724427Z" level=info msg="Starting up"
Jan 10 13:01:13 minikube dockerd[1039]: time="2024-01-10T13:01:13.093243427Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jan 10 13:01:13 minikube dockerd[1039]: time="2024-01-10T13:01:13.098377261Z" level=info msg="Loading containers: start."
Jan 10 13:01:13 minikube dockerd[1039]: time="2024-01-10T13:01:13.128382969Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Jan 10 13:01:13 minikube dockerd[1039]: time="2024-01-10T13:01:13.140077052Z" level=info msg="Loading containers: done."
Jan 10 13:01:13 minikube dockerd[1039]: time="2024-01-10T13:01:13.143497344Z" level=info msg="Docker daemon" commit=311b9ff graphdriver=overlay2 version=24.0.7
Jan 10 13:01:13 minikube dockerd[1039]: time="2024-01-10T13:01:13.143518552Z" level=info msg="Daemon has completed initialization"
Jan 10 13:01:13 minikube dockerd[1039]: time="2024-01-10T13:01:13.158898386Z" level=info msg="API listen on [::]:2376"
Jan 10 13:01:13 minikube dockerd[1039]: time="2024-01-10T13:01:13.158904094Z" level=info msg="API listen on /var/run/docker.sock"
Jan 10 13:01:13 minikube systemd[1]: Started Docker Application Container Engine.
Jan 10 13:01:13 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jan 10 13:01:13 minikube cri-dockerd[1261]: time="2024-01-10T13:01:13Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jan 10 13:01:13 minikube cri-dockerd[1261]: time="2024-01-10T13:01:13Z" level=info msg="Start docker client with request timeout 0s"
Jan 10 13:01:13 minikube cri-dockerd[1261]: time="2024-01-10T13:01:13Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jan 10 13:01:13 minikube cri-dockerd[1261]: time="2024-01-10T13:01:13Z" level=info msg="Loaded network plugin cni"
Jan 10 13:01:13 minikube cri-dockerd[1261]: time="2024-01-10T13:01:13Z" level=info msg="Docker cri networking managed by network plugin cni"
Jan 10 13:01:13 minikube cri-dockerd[1261]: time="2024-01-10T13:01:13Z" level=info msg="Docker Info: &{ID:a2ddae48-234b-4661-9ccd-794395a2873f Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:8 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:[] Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:[] Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6tables:true Debug:false NFd:24 OomKillDisable:false NGoroutines:35 SystemTime:2024-01-10T13:01:13.377689928Z LoggingDriver:json-file CgroupDriver:cgroupfs CgroupVersion:2 NEventsListener:0 KernelVersion:6.5.11-linuxkit OperatingSystem:Ubuntu 22.04.3 LTS OSVersion:22.04 OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:0x40001d31f0 NCPU:8 MemTotal:8227811328 GenericResources:[] DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy:control-plane.minikube.internal Name:minikube Labels:[provider=docker] ExperimentalBuild:false ServerVersion:24.0.7 ClusterStore: ClusterAdvertise: Runtimes:map[io.containerd.runc.v2:{Path:runc Args:[] Shim:<nil>} runc:{Path:runc Args:[] Shim:<nil>}] DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:[] Nodes:0 Managers:0 Cluster:<nil> Warnings:[]} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: DefaultAddressPools:[] Warnings:[]}"
Jan 10 13:01:13 minikube cri-dockerd[1261]: time="2024-01-10T13:01:13Z" level=info msg="Setting cgroupDriver cgroupfs"
Jan 10 13:01:13 minikube cri-dockerd[1261]: time="2024-01-10T13:01:13Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jan 10 13:01:13 minikube cri-dockerd[1261]: time="2024-01-10T13:01:13Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jan 10 13:01:13 minikube cri-dockerd[1261]: time="2024-01-10T13:01:13Z" level=info msg="Start cri-dockerd grpc backend"
Jan 10 13:01:13 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jan 10 13:01:16 minikube cri-dockerd[1261]: time="2024-01-10T13:01:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f027fec1872965da373f923d26f061bc32310388ff5a0973caaa2d0eda8fc82f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 10 13:01:16 minikube cri-dockerd[1261]: time="2024-01-10T13:01:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d36d88c486bd2c073412d01d70a61da3219a845bdb1882988d87f439ee7e500b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 10 13:01:16 minikube cri-dockerd[1261]: time="2024-01-10T13:01:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/68f14426acf2e956ec39367ddeabbf62c8afee39adb854e3ef6403580944d68e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 10 13:01:16 minikube cri-dockerd[1261]: time="2024-01-10T13:01:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/558be46e551a11b94b9fe39ab7f8813dab8641b73dba183c39fdc88392da3831/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 10 13:01:33 minikube cri-dockerd[1261]: time="2024-01-10T13:01:33Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e84c26beb34abf98716146b490705414274a10426f7e86f544d81d69371d3553/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 10 13:01:34 minikube cri-dockerd[1261]: time="2024-01-10T13:01:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/913e3b18dd639317c9e36e16980f1f360a493644edef460c7eeaeab05c15d42b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 10 13:01:34 minikube cri-dockerd[1261]: time="2024-01-10T13:01:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f2ecfda74fbb2f588a66eea1cc22c14c9b6731823da325cabe30e6f4d2990e3e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 10 13:01:34 minikube cri-dockerd[1261]: time="2024-01-10T13:01:34Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/23faf3e799cf99e85020ac79f77830c5d65fc8ef906e9124e74a8a2ab568a3b8/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 10 13:01:34 minikube cri-dockerd[1261]: time="2024-01-10T13:01:34Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-8qzxk_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Jan 10 13:01:34 minikube cri-dockerd[1261]: time="2024-01-10T13:01:34Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-5dd5756b68-8qzxk_kube-system\": unexpected command output Device \"eth0\" does not exist.\n with error: exit status 1"
Jan 10 13:01:39 minikube cri-dockerd[1261]: time="2024-01-10T13:01:39Z" level=info msg="Stop pulling image docker.io/kindest/kindnetd:v20230809-80a64d96: Status: Downloaded newer image for kindest/kindnetd:v20230809-80a64d96"
Jan 10 13:01:40 minikube cri-dockerd[1261]: time="2024-01-10T13:01:40Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jan 10 13:01:47 minikube dockerd[1039]: time="2024-01-10T13:01:47.282252346Z" level=info msg="ignoring event" container=40794184865afc76b1725b09a76a6504dd3de8eee79a763bcb2be1797310cf4a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 10 13:01:47 minikube dockerd[1039]: time="2024-01-10T13:01:47.306869179Z" level=info msg="ignoring event" container=23faf3e799cf99e85020ac79f77830c5d65fc8ef906e9124e74a8a2ab568a3b8 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 10 13:01:47 minikube cri-dockerd[1261]: time="2024-01-10T13:01:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0e04a4d3db6b8b8f9bb0c6deb80a5384af7b2666a1690ea3669eb384f3ef02d9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jan 10 13:02:03 minikube dockerd[1039]: time="2024-01-10T13:02:03.482405173Z" level=info msg="ignoring event" container=81fc5e0ebcc1f1d8cf54db01713237d9d538911ea1b7914e4405ae1d38807d30 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 10 13:07:58 minikube cri-dockerd[1261]: time="2024-01-10T13:07:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/70f31240c35bd9dd1aafd97161019bd39699ed0fa6445821ea1138822f21908f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 10 13:08:00 minikube dockerd[1039]: time="2024-01-10T13:08:00.873077463Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 10 13:08:00 minikube dockerd[1039]: time="2024-01-10T13:08:00.873150213Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 10 13:08:18 minikube dockerd[1039]: time="2024-01-10T13:08:18.040040888Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 10 13:08:18 minikube dockerd[1039]: time="2024-01-10T13:08:18.040095138Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 10 13:08:44 minikube dockerd[1039]: time="2024-01-10T13:08:44.126936428Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 10 13:08:44 minikube dockerd[1039]: time="2024-01-10T13:08:44.127063095Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 10 13:09:35 minikube dockerd[1039]: time="2024-01-10T13:09:35.109953632Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 10 13:09:35 minikube dockerd[1039]: time="2024-01-10T13:09:35.110056632Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 10 13:11:02 minikube dockerd[1039]: time="2024-01-10T13:11:02.801423797Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 10 13:11:02 minikube dockerd[1039]: time="2024-01-10T13:11:02.801678881Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 10 13:13:52 minikube dockerd[1039]: time="2024-01-10T13:13:52.126172459Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 10 13:13:52 minikube dockerd[1039]: time="2024-01-10T13:13:52.126286709Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 10 13:18:56 minikube dockerd[1039]: time="2024-01-10T13:18:56.215730794Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Jan 10 13:18:56 minikube dockerd[1039]: time="2024-01-10T13:18:56.215968711Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Jan 10 13:22:06 minikube dockerd[1039]: time="2024-01-10T13:22:06.833440258Z" level=info msg="ignoring event" container=70f31240c35bd9dd1aafd97161019bd39699ed0fa6445821ea1138822f21908f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 10 14:34:16 minikube cri-dockerd[1261]: time="2024-01-10T14:34:16Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d8b7af146520990d4cb1e4daf5571f667a079e6a89aff56cb8ddd9e2dbaf5e2a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jan 10 14:34:25 minikube cri-dockerd[1261]: time="2024-01-10T14:34:25Z" level=info msg="Stop pulling image pashadigori2000/cloud-computing:frontend-image-v1: Status: Downloaded newer image for pashadigori2000/cloud-computing:frontend-image-v1"
Jan 10 14:35:10 minikube dockerd[1039]: time="2024-01-10T14:35:10.334264509Z" level=info msg="ignoring event" container=0ea6614b6db7dc61fa7aa26687ff49576611b32debabd2751c3afb8d0790d87e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jan 10 14:35:10 minikube dockerd[1039]: time="2024-01-10T14:35:10.385308967Z" level=info msg="ignoring event" container=d8b7af146520990d4cb1e4daf5571f667a079e6a89aff56cb8ddd9e2dbaf5e2a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                      CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
e9fb4b1333874       ba04bb24b9575                                                                              9 hours ago         Running             storage-provisioner       1                   e84c26beb34ab       storage-provisioner
97d2624317267       97e04611ad434                                                                              9 hours ago         Running             coredns                   1                   0e04a4d3db6b8       coredns-5dd5756b68-8qzxk
03ec8712e80c0       kindest/kindnetd@sha256:4a58d1cd2b45bf2460762a51a4aa9c80861f460af35800c05baab0573f923052   9 hours ago         Running             kindnet-cni               0                   f2ecfda74fbb2       kindnet-6fcx9
40794184865af       97e04611ad434                                                                              9 hours ago         Exited              coredns                   0                   23faf3e799cf9       coredns-5dd5756b68-8qzxk
df25cf8ca18f3       a5dd5cdd6d3ef                                                                              9 hours ago         Running             kube-proxy                0                   913e3b18dd639       kube-proxy-4z9zh
81fc5e0ebcc1f       ba04bb24b9575                                                                              9 hours ago         Exited              storage-provisioner       0                   e84c26beb34ab       storage-provisioner
e924dd11e6c24       537e9a59ee2fd                                                                              9 hours ago         Running             kube-apiserver            0                   558be46e551a1       kube-apiserver-minikube
dd119bd7bc042       8276439b4f237                                                                              9 hours ago         Running             kube-controller-manager   0                   68f14426acf2e       kube-controller-manager-minikube
a1bb21f8e0578       9cdd6470f48c8                                                                              9 hours ago         Running             etcd                      0                   d36d88c486bd2       etcd-minikube
0d315ed67906b       42a4e73724daa                                                                              9 hours ago         Running             kube-scheduler            0                   f027fec187296       kube-scheduler-minikube

* 
* ==> coredns [40794184865a] <==
* [INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.10.1
linux/arm64, go1.20, 055b2c3
[INFO] plugin/health: Going into lameduck mode for 5s
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: connect: network is unreachable
[INFO] 127.0.0.1:38316 - 60388 "HINFO IN 343462917251388645.4082516048538609637. udp 56 false 512" - - 0 5.000199336s
[ERROR] plugin/errors: 2 343462917251388645.4082516048538609637. HINFO: dial udp 192.168.65.254:53: connect: network is unreachable
[INFO] 127.0.0.1:48006 - 29355 "HINFO IN 343462917251388645.4082516048538609637. udp 56 false 512" - - 0 5.000031836s
[ERROR] plugin/errors: 2 343462917251388645.4082516048538609637. HINFO: dial udp 192.168.65.254:53: connect: network is unreachable

* 
* ==> coredns [97d262431726] <==
* [INFO] 10.244.1.8:46204 - 27005 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000872791s
[INFO] 10.244.1.8:46204 - 27386 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.001090125s
[INFO] 10.244.1.8:48203 - 63891 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000842833s
[INFO] 10.244.1.8:48203 - 64058 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000978416s
[INFO] 10.244.1.8:36149 - 8777 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000788s
[INFO] 10.244.1.8:36149 - 8986 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000917083s
[INFO] 10.244.1.8:44305 - 51043 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.001039292s
[INFO] 10.244.1.8:44305 - 50834 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.001103125s
[INFO] 10.244.1.8:60519 - 34332 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000923625s
[INFO] 10.244.1.8:60519 - 34082 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.001014709s
[INFO] 10.244.1.8:49482 - 58057 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.00071925s
[INFO] 10.244.1.8:49482 - 57807 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000982125s
[INFO] 10.244.1.8:38707 - 36976 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000626167s
[INFO] 10.244.1.8:38707 - 36768 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000735625s
[INFO] 10.244.1.8:33183 - 5035 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000769084s
[INFO] 10.244.1.8:33183 - 5327 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.0008875s
[INFO] 10.244.1.8:42423 - 45245 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000742625s
[INFO] 10.244.1.8:42423 - 45495 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000859459s
[INFO] 10.244.1.8:55308 - 49156 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000740541s
[INFO] 10.244.1.8:55308 - 48989 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000828875s
[INFO] 10.244.1.8:35436 - 2211 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000669833s
[INFO] 10.244.1.8:35436 - 2377 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000797375s
[INFO] 10.244.1.8:55684 - 54512 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.00059775s
[INFO] 10.244.1.8:55684 - 54804 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.0007155s
[INFO] 10.244.1.8:40303 - 60974 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.00088525s
[INFO] 10.244.1.8:40303 - 61397 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.001001417s
[INFO] 10.244.1.8:47610 - 24782 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000659833s
[INFO] 10.244.1.8:47610 - 24907 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000781292s
[INFO] 10.244.1.8:40674 - 40762 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000794209s
[INFO] 10.244.1.8:40674 - 41095 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000920666s
[INFO] 10.244.1.8:47944 - 61475 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.00087175s
[INFO] 10.244.1.8:47944 - 61308 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000920333s
[INFO] 10.244.1.8:38484 - 43851 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000862917s
[INFO] 10.244.1.8:38484 - 43684 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.0009925s
[INFO] 10.244.1.8:49316 - 34458 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000664291s
[INFO] 10.244.1.8:49316 - 34250 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000780667s
[INFO] 10.244.1.8:46084 - 37352 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000556458s
[INFO] 10.244.1.8:46084 - 36929 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000673583s
[INFO] 10.244.1.10:55636 - 57211 "AAAA IN backend-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 152 0.000682458s
[INFO] 10.244.1.10:55636 - 28259 "A IN backend-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000703208s
[INFO] 10.244.1.8:49894 - 58580 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000534083s
[INFO] 10.244.1.8:49894 - 58157 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000613s
[INFO] 10.244.1.8:55662 - 29028 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.009867125s
[INFO] 10.244.1.8:55662 - 28862 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.009874875s
[INFO] 10.244.1.8:54038 - 410 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000513792s
[INFO] 10.244.1.8:54038 - 244 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.00062775s
[INFO] 10.244.1.8:49875 - 58624 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.00068125s
[INFO] 10.244.1.8:49875 - 58374 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000805792s
[INFO] 10.244.1.8:42870 - 63900 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.001015959s
[INFO] 10.244.1.8:42870 - 64067 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.001082875s
[INFO] 10.244.1.8:45439 - 37802 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000571625s
[INFO] 10.244.1.8:45439 - 38011 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000672s
[INFO] 10.244.1.8:51885 - 59146 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000922958s
[INFO] 10.244.1.8:51885 - 59527 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000978917s
[INFO] 10.244.1.8:44415 - 47675 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000827042s
[INFO] 10.244.1.8:44415 - 48097 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000952875s
[INFO] 10.244.1.8:43037 - 4598 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000766625s
[INFO] 10.244.1.8:43037 - 4217 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000829625s
[INFO] 10.244.1.8:38030 - 12171 "A IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 112 0.000700291s
[INFO] 10.244.1.8:38030 - 12421 "AAAA IN mysql-service.default.svc.cluster.local. udp 57 false 512" NOERROR qr,aa,rd 150 0.000825083s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=8220a6eb95f0a4d75f7f2d7b14cef975f050512d
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_01_10T15_01_20_0700
                    minikube.k8s.io/version=v1.32.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 10 Jan 2024 13:01:17 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 10 Jan 2024 21:57:30 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 10 Jan 2024 21:55:57 +0000   Wed, 10 Jan 2024 13:01:16 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 10 Jan 2024 21:55:57 +0000   Wed, 10 Jan 2024 13:01:16 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 10 Jan 2024 21:55:57 +0000   Wed, 10 Jan 2024 13:01:16 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 10 Jan 2024 21:55:57 +0000   Wed, 10 Jan 2024 13:01:18 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  61202244Ki
  memory:             8034972Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  61202244Ki
  memory:             8034972Ki
  pods:               110
System Info:
  Machine ID:                 25083dd4c35d4286a9155166e3110133
  System UUID:                25083dd4c35d4286a9155166e3110133
  Boot ID:                    44e80595-91c4-475e-a402-48afa069270c
  Kernel Version:             6.5.11-linuxkit
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-5dd5756b68-8qzxk            100m (1%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (2%!)(MISSING)     8h
  kube-system                 etcd-minikube                       100m (1%!)(MISSING)     0 (0%!)(MISSING)      100Mi (1%!)(MISSING)       0 (0%!)(MISSING)         8h
  kube-system                 kindnet-6fcx9                       100m (1%!)(MISSING)     100m (1%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      8h
  kube-system                 kube-apiserver-minikube             250m (3%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8h
  kube-system                 kube-controller-manager-minikube    200m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8h
  kube-system                 kube-proxy-4z9zh                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8h
  kube-system                 kube-scheduler-minikube             100m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8h
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%!)(MISSING)  100m (1%!)(MISSING)
  memory             220Mi (2%!)(MISSING)  220Mi (2%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:              <none>


Name:               minikube-m02
Roles:              <none>
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube-m02
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 10 Jan 2024 13:01:34 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m02
  AcquireTime:     <unset>
  RenewTime:       Wed, 10 Jan 2024 21:57:37 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 10 Jan 2024 21:56:24 +0000   Wed, 10 Jan 2024 13:01:34 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 10 Jan 2024 21:56:24 +0000   Wed, 10 Jan 2024 13:01:34 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 10 Jan 2024 21:56:24 +0000   Wed, 10 Jan 2024 13:01:34 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 10 Jan 2024 21:56:24 +0000   Wed, 10 Jan 2024 13:01:34 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.3
  Hostname:    minikube-m02
Capacity:
  cpu:                8
  ephemeral-storage:  61202244Ki
  memory:             8034972Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  61202244Ki
  memory:             8034972Ki
  pods:               110
System Info:
  Machine ID:                 0da05e2d84d642af988562225961b188
  System UUID:                0da05e2d84d642af988562225961b188
  Boot ID:                    44e80595-91c4-475e-a402-48afa069270c
  Kernel Version:             6.5.11-linuxkit
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.1.0/24
PodCIDRs:                     10.244.1.0/24
Non-terminated Pods:          (3 in total)
  Namespace                   Name                                            CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                            ------------  ----------  ---------------  -------------  ---
  default                     authentification-deployment-5484bdc86c-nq47t    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7h32m
  kube-system                 kindnet-jbqq8                                   100m (1%!)(MISSING)     100m (1%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      8h
  kube-system                 kube-proxy-xncnx                                0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (1%!)(MISSING)  100m (1%!)(MISSING)
  memory             50Mi (0%!)(MISSING)  50Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)     0 (0%!)(MISSING)
Events:              <none>


Name:               minikube-m03
Roles:              <none>
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube-m03
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: /var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 10 Jan 2024 13:01:44 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m03
  AcquireTime:     <unset>
  RenewTime:       Wed, 10 Jan 2024 21:57:32 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 10 Jan 2024 21:55:43 +0000   Wed, 10 Jan 2024 13:01:44 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 10 Jan 2024 21:55:43 +0000   Wed, 10 Jan 2024 13:01:44 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 10 Jan 2024 21:55:43 +0000   Wed, 10 Jan 2024 13:01:44 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 10 Jan 2024 21:55:43 +0000   Wed, 10 Jan 2024 13:01:44 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.4
  Hostname:    minikube-m03
Capacity:
  cpu:                8
  ephemeral-storage:  61202244Ki
  memory:             8034972Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  61202244Ki
  memory:             8034972Ki
  pods:               110
System Info:
  Machine ID:                 8fa7375073f14919a83ca91ba580243b
  System UUID:                8fa7375073f14919a83ca91ba580243b
  Boot ID:                    44e80595-91c4-475e-a402-48afa069270c
  Kernel Version:             6.5.11-linuxkit
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://24.0.7
  Kubelet Version:            v1.28.3
  Kube-Proxy Version:         v1.28.3
PodCIDR:                      10.244.2.0/24
PodCIDRs:                     10.244.2.0/24
Non-terminated Pods:          (5 in total)
  Namespace                   Name                                    CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                    ------------  ----------  ---------------  -------------  ---
  default                     backend-deployment-55ff5fbff7-7xw68     0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         7h31m
  default                     frontend-deployment-576464797f-bbssl    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         26m
  default                     mysql-deployment-5dbc7f9b6b-n4gmn       0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8h
  kube-system                 kindnet-ggc2f                           100m (1%!)(MISSING)     100m (1%!)(MISSING)   50Mi (0%!)(MISSING)        50Mi (0%!)(MISSING)      8h
  kube-system                 kube-proxy-n4b57                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         8h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (1%!)(MISSING)  100m (1%!)(MISSING)
  memory             50Mi (0%!)(MISSING)  50Mi (0%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)     0 (0%!)(MISSING)
Events:              <none>

* 
* ==> dmesg <==
* [Jan10 16:32] cacheinfo: Unable to detect cache hierarchy for CPU 0
[  +0.074283] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.100574] netlink: 'init': attribute type 4 has an invalid length.
[  +0.068672] grpcfuse: loading out-of-tree module taints kernel.
[Jan10 16:33] systemd[1199]: memfd_create() called without MFD_EXEC or MFD_NOEXEC_SEAL set
[Jan10 17:53] hrtimer: interrupt took 3237083 ns

* 
* ==> etcd [a1bb21f8e057] <==
* {"level":"info","ts":"2024-01-10T20:27:22.832258Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15274}
{"level":"info","ts":"2024-01-10T20:27:22.836758Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15274,"took":"3.683791ms","hash":703416567}
{"level":"info","ts":"2024-01-10T20:27:22.836824Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":703416567,"revision":15274,"compact-revision":14962}
{"level":"info","ts":"2024-01-10T20:32:22.843122Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15586}
{"level":"info","ts":"2024-01-10T20:32:22.847174Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15586,"took":"3.205417ms","hash":2975693437}
{"level":"info","ts":"2024-01-10T20:32:22.847242Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2975693437,"revision":15586,"compact-revision":15274}
{"level":"info","ts":"2024-01-10T20:37:22.850094Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15899}
{"level":"info","ts":"2024-01-10T20:37:22.853402Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":15899,"took":"2.368ms","hash":2464095397}
{"level":"info","ts":"2024-01-10T20:37:22.853467Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2464095397,"revision":15899,"compact-revision":15586}
{"level":"info","ts":"2024-01-10T20:42:22.856209Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16212}
{"level":"info","ts":"2024-01-10T20:42:22.860141Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":16212,"took":"3.213416ms","hash":3796651576}
{"level":"info","ts":"2024-01-10T20:42:22.860198Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3796651576,"revision":16212,"compact-revision":15899}
{"level":"info","ts":"2024-01-10T20:47:22.861566Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16524}
{"level":"info","ts":"2024-01-10T20:47:22.864974Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":16524,"took":"2.649833ms","hash":3550094475}
{"level":"info","ts":"2024-01-10T20:47:22.865046Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3550094475,"revision":16524,"compact-revision":16212}
{"level":"info","ts":"2024-01-10T20:47:29.362313Z","caller":"etcdserver/server.go:1395","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000}
{"level":"info","ts":"2024-01-10T20:47:29.369954Z","caller":"etcdserver/server.go:2413","msg":"saved snapshot","snapshot-index":20002}
{"level":"info","ts":"2024-01-10T20:47:29.370418Z","caller":"etcdserver/server.go:2443","msg":"compacted Raft logs","compact-index":15002}
{"level":"info","ts":"2024-01-10T20:52:22.87961Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16836}
{"level":"info","ts":"2024-01-10T20:52:22.883635Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":16836,"took":"3.32825ms","hash":2082600540}
{"level":"info","ts":"2024-01-10T20:52:22.883697Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2082600540,"revision":16836,"compact-revision":16524}
{"level":"info","ts":"2024-01-10T20:57:22.895274Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17149}
{"level":"info","ts":"2024-01-10T20:57:22.900316Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":17149,"took":"3.95525ms","hash":1528491600}
{"level":"info","ts":"2024-01-10T20:57:22.900388Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1528491600,"revision":17149,"compact-revision":16836}
{"level":"info","ts":"2024-01-10T21:02:22.904056Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17463}
{"level":"info","ts":"2024-01-10T21:02:22.908377Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":17463,"took":"3.546292ms","hash":3687406366}
{"level":"info","ts":"2024-01-10T21:02:22.908446Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3687406366,"revision":17463,"compact-revision":17149}
{"level":"info","ts":"2024-01-10T21:07:22.913991Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17775}
{"level":"info","ts":"2024-01-10T21:07:22.919444Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":17775,"took":"3.988958ms","hash":2670730090}
{"level":"info","ts":"2024-01-10T21:07:22.919525Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2670730090,"revision":17775,"compact-revision":17463}
{"level":"info","ts":"2024-01-10T21:12:22.922356Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18102}
{"level":"info","ts":"2024-01-10T21:12:22.926004Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":18102,"took":"3.047917ms","hash":2841589893}
{"level":"info","ts":"2024-01-10T21:12:22.926058Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2841589893,"revision":18102,"compact-revision":17775}
{"level":"info","ts":"2024-01-10T21:17:22.878808Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18449}
{"level":"info","ts":"2024-01-10T21:17:22.882927Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":18449,"took":"3.343584ms","hash":428842977}
{"level":"info","ts":"2024-01-10T21:17:22.882999Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":428842977,"revision":18449,"compact-revision":18102}
{"level":"info","ts":"2024-01-10T21:22:22.881259Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18761}
{"level":"info","ts":"2024-01-10T21:22:22.885766Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":18761,"took":"3.69725ms","hash":756962037}
{"level":"info","ts":"2024-01-10T21:22:22.885841Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":756962037,"revision":18761,"compact-revision":18449}
{"level":"info","ts":"2024-01-10T21:27:22.887666Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19075}
{"level":"info","ts":"2024-01-10T21:27:22.893838Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":19075,"took":"3.8785ms","hash":401158939}
{"level":"info","ts":"2024-01-10T21:27:22.893931Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":401158939,"revision":19075,"compact-revision":18761}
{"level":"info","ts":"2024-01-10T21:32:22.892112Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19387}
{"level":"info","ts":"2024-01-10T21:32:22.896957Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":19387,"took":"4.024875ms","hash":4174352816}
{"level":"info","ts":"2024-01-10T21:32:22.897035Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4174352816,"revision":19387,"compact-revision":19075}
{"level":"info","ts":"2024-01-10T21:37:22.967307Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19734}
{"level":"info","ts":"2024-01-10T21:37:22.972345Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":19734,"took":"3.76825ms","hash":1223645744}
{"level":"info","ts":"2024-01-10T21:37:22.972429Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1223645744,"revision":19734,"compact-revision":19387}
{"level":"info","ts":"2024-01-10T21:42:22.97608Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20042}
{"level":"info","ts":"2024-01-10T21:42:22.980445Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":20042,"took":"3.547542ms","hash":1260249317}
{"level":"info","ts":"2024-01-10T21:42:22.980514Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1260249317,"revision":20042,"compact-revision":19734}
{"level":"info","ts":"2024-01-10T21:47:22.988122Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20349}
{"level":"info","ts":"2024-01-10T21:47:22.994054Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":20349,"took":"4.092834ms","hash":2782671331}
{"level":"info","ts":"2024-01-10T21:47:22.994129Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2782671331,"revision":20349,"compact-revision":20042}
{"level":"info","ts":"2024-01-10T21:52:22.997777Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20661}
{"level":"info","ts":"2024-01-10T21:52:23.003147Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":20661,"took":"3.976542ms","hash":3706594403}
{"level":"info","ts":"2024-01-10T21:52:23.00322Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3706594403,"revision":20661,"compact-revision":20349}
{"level":"info","ts":"2024-01-10T21:57:23.007012Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20974}
{"level":"info","ts":"2024-01-10T21:57:23.011516Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":20974,"took":"3.281166ms","hash":2498641931}
{"level":"info","ts":"2024-01-10T21:57:23.011577Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":2498641931,"revision":20974,"compact-revision":20661}

* 
* ==> kernel <==
*  21:57:38 up  5:24,  0 users,  load average: 0.15, 0.36, 0.46
Linux minikube 6.5.11-linuxkit #1 SMP PREEMPT Wed Dec  6 17:08:31 UTC 2023 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.3 LTS"

* 
* ==> kindnet [03ec8712e80c] <==
* I0110 21:56:07.708188       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0110 21:56:07.708279       1 main.go:227] handling current node
I0110 21:56:07.708298       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0110 21:56:07.708305       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0110 21:56:07.708444       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0110 21:56:07.708470       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0110 21:56:17.719443       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0110 21:56:17.719529       1 main.go:227] handling current node
I0110 21:56:17.719563       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0110 21:56:17.719576       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0110 21:56:17.719897       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0110 21:56:17.719932       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0110 21:56:27.734763       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0110 21:56:27.734881       1 main.go:227] handling current node
I0110 21:56:27.734912       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0110 21:56:27.734923       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0110 21:56:27.735244       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0110 21:56:27.735283       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0110 21:56:37.741682       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0110 21:56:37.741743       1 main.go:227] handling current node
I0110 21:56:37.741766       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0110 21:56:37.741771       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0110 21:56:37.741971       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0110 21:56:37.741984       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0110 21:56:47.762389       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0110 21:56:47.762450       1 main.go:227] handling current node
I0110 21:56:47.762477       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0110 21:56:47.762488       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0110 21:56:47.762707       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0110 21:56:47.762736       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0110 21:56:57.873893       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0110 21:56:57.874025       1 main.go:227] handling current node
I0110 21:56:57.874085       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0110 21:56:57.874113       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0110 21:56:57.874575       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0110 21:56:57.874618       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0110 21:57:07.887690       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0110 21:57:07.887795       1 main.go:227] handling current node
I0110 21:57:07.887817       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0110 21:57:07.887826       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0110 21:57:07.888182       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0110 21:57:07.888195       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0110 21:57:17.903497       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0110 21:57:17.903604       1 main.go:227] handling current node
I0110 21:57:17.903635       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0110 21:57:17.903645       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0110 21:57:17.903920       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0110 21:57:17.903944       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0110 21:57:27.912377       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0110 21:57:27.912426       1 main.go:227] handling current node
I0110 21:57:27.912454       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0110 21:57:27.912473       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0110 21:57:27.912665       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0110 21:57:27.912700       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0110 21:57:37.926997       1 main.go:223] Handling node with IPs: map[192.168.49.2:{}]
I0110 21:57:37.927129       1 main.go:227] handling current node
I0110 21:57:37.927178       1 main.go:223] Handling node with IPs: map[192.168.49.3:{}]
I0110 21:57:37.927194       1 main.go:250] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0110 21:57:37.927427       1 main.go:223] Handling node with IPs: map[192.168.49.4:{}]
I0110 21:57:37.927451       1 main.go:250] Node minikube-m03 has CIDR [10.244.2.0/24] 

* 
* ==> kube-apiserver [e924dd11e6c2] <==
* I0110 13:01:17.804916       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0110 13:01:17.804923       1 crd_finalizer.go:266] Starting CRDFinalizer
I0110 13:01:17.805019       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0110 13:01:17.805286       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0110 13:01:17.806000       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0110 13:01:17.806790       1 apf_controller.go:372] Starting API Priority and Fairness config controller
I0110 13:01:17.806803       1 controller.go:78] Starting OpenAPI AggregationController
I0110 13:01:17.866244       1 shared_informer.go:318] Caches are synced for node_authorizer
I0110 13:01:17.904778       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0110 13:01:17.905267       1 controller.go:624] quota admission added evaluator for: namespaces
I0110 13:01:17.905483       1 shared_informer.go:318] Caches are synced for cluster_authentication_trust_controller
I0110 13:01:17.905518       1 shared_informer.go:318] Caches are synced for crd-autoregister
I0110 13:01:17.905535       1 aggregator.go:166] initial CRD sync complete...
I0110 13:01:17.905539       1 autoregister_controller.go:141] Starting autoregister controller
I0110 13:01:17.905542       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0110 13:01:17.905545       1 cache.go:39] Caches are synced for autoregister controller
I0110 13:01:17.905547       1 shared_informer.go:318] Caches are synced for configmaps
I0110 13:01:17.906017       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0110 13:01:17.906824       1 apf_controller.go:377] Running API Priority and Fairness config worker
I0110 13:01:17.906831       1 apf_controller.go:380] Running API Priority and Fairness periodic rebalancing process
I0110 13:01:17.911905       1 controller.go:624] quota admission added evaluator for: leases.coordination.k8s.io
I0110 13:01:18.811018       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0110 13:01:18.815034       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0110 13:01:18.815058       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0110 13:01:18.946170       1 controller.go:624] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0110 13:01:18.955321       1 controller.go:624] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0110 13:01:19.009975       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0110 13:01:19.011644       1 lease.go:263] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0110 13:01:19.011960       1 controller.go:624] quota admission added evaluator for: endpoints
I0110 13:01:19.013365       1 controller.go:624] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0110 13:01:19.889817       1 controller.go:624] quota admission added evaluator for: serviceaccounts
I0110 13:01:20.168217       1 controller.go:624] quota admission added evaluator for: deployments.apps
I0110 13:01:20.171559       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0110 13:01:20.174927       1 controller.go:624] quota admission added evaluator for: daemonsets.apps
I0110 13:01:33.144233       1 controller.go:624] quota admission added evaluator for: replicasets.apps
I0110 13:01:33.693273       1 controller.go:624] quota admission added evaluator for: controllerrevisions.apps
I0110 13:04:48.463004       1 alloc.go:330] "allocated clusterIPs" service="default/mysql-service" clusterIPs={"IPv4":"10.101.42.75"}
I0110 13:07:58.408101       1 alloc.go:330] "allocated clusterIPs" service="default/authentification-service" clusterIPs={"IPv4":"10.102.89.96"}
I0110 14:25:39.031116       1 alloc.go:330] "allocated clusterIPs" service="default/backend-service" clusterIPs={"IPv4":"10.106.83.105"}
I0110 14:34:16.255788       1 alloc.go:330] "allocated clusterIPs" service="default/frontend-service" clusterIPs={"IPv4":"10.109.109.205"}
E0110 15:51:30.823129       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 15:51:30.823129       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 15:51:30.823129       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 15:51:30.836769       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 17:24:05.476261       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 17:24:05.476261       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 17:24:05.476296       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 17:24:05.493959       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 17:24:17.075080       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 17:24:17.075118       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 18:46:43.248316       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 18:46:43.248317       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 18:46:43.248772       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 18:46:43.258021       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 19:48:12.239842       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 19:48:12.239843       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 19:51:39.882982       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 19:51:39.883007       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 19:51:39.883112       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0110 19:51:39.892231       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"

* 
* ==> kube-controller-manager [dd119bd7bc04] <==
* I0110 21:11:19.831759       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="55.292¬µs"
I0110 21:11:34.428872       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="296.917¬µs"
I0110 21:16:24.413729       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="9.181167ms"
I0110 21:16:24.414158       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="52.75¬µs"
I0110 21:16:34.512648       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="9.920292ms"
I0110 21:16:34.512703       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="28.875¬µs"
I0110 21:16:49.353027       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="37.625¬µs"
I0110 21:21:40.977878       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="6.400833ms"
I0110 21:21:40.977951       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="26.875¬µs"
I0110 21:21:51.055673       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="5.472125ms"
I0110 21:21:51.057235       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="62.334¬µs"
I0110 21:22:06.366808       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="208¬µs"
I0110 21:26:54.711652       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="12.948792ms"
I0110 21:26:54.712438       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="48.084¬µs"
I0110 21:27:05.831776       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="8.259541ms"
I0110 21:27:05.831984       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="125.125¬µs"
I0110 21:27:20.366604       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="187.125¬µs"
I0110 21:31:38.192532       1 event.go:307] "Event occurred" object="default/frontend-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set frontend-deployment-576464797f to 1"
I0110 21:31:38.201102       1 event.go:307] "Event occurred" object="default/frontend-deployment-576464797f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: frontend-deployment-576464797f-bbssl"
I0110 21:31:38.203615       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-deployment-576464797f" duration="11.250833ms"
I0110 21:31:38.211393       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-deployment-576464797f" duration="7.74575ms"
I0110 21:31:38.222782       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-deployment-576464797f" duration="11.356792ms"
I0110 21:31:38.222835       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-deployment-576464797f" duration="22.292¬µs"
I0110 21:31:38.224892       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-deployment-576464797f" duration="31.541¬µs"
I0110 21:31:39.213257       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-deployment-576464797f" duration="5.214833ms"
I0110 21:31:39.213364       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-deployment-576464797f" duration="25.875¬µs"
I0110 21:31:39.223963       1 event.go:307] "Event occurred" object="default/frontend-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled down replica set frontend-deployment-5574466cf6 to 0 from 1"
I0110 21:31:39.233032       1 event.go:307] "Event occurred" object="default/frontend-deployment-5574466cf6" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: frontend-deployment-5574466cf6-ltk6v"
I0110 21:31:39.246200       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-deployment-5574466cf6" duration="22.059875ms"
I0110 21:31:39.255698       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-deployment-5574466cf6" duration="9.465084ms"
I0110 21:31:39.255844       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-deployment-5574466cf6" duration="24.209¬µs"
I0110 21:31:39.346834       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-deployment-5574466cf6" duration="40.458¬µs"
I0110 21:31:40.217217       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-deployment-5574466cf6" duration="41.25¬µs"
I0110 21:31:40.223940       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-deployment-5574466cf6" duration="282.5¬µs"
I0110 21:31:40.226574       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/frontend-deployment-5574466cf6" duration="57.791¬µs"
I0110 21:32:07.493065       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="12.135833ms"
I0110 21:32:07.493984       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="539.542¬µs"
I0110 21:32:17.591368       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="7.818334ms"
I0110 21:32:17.592835       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="53.708¬µs"
I0110 21:32:28.358223       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="165.375¬µs"
I0110 21:37:20.924008       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="3.741083ms"
I0110 21:37:20.924396       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="214.917¬µs"
I0110 21:37:31.005019       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="5.544583ms"
I0110 21:37:31.005587       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="332.833¬µs"
I0110 21:37:45.416943       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="132.958¬µs"
I0110 21:42:42.292171       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="12.602417ms"
I0110 21:42:42.292948       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="49.333¬µs"
I0110 21:42:52.364268       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="7.276875ms"
I0110 21:42:52.364351       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="47.25¬µs"
I0110 21:43:07.424103       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="168.167¬µs"
I0110 21:47:57.938145       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="17.3945ms"
I0110 21:47:57.938264       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="39.916¬µs"
I0110 21:48:09.059587       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="13.35025ms"
I0110 21:48:09.059759       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="102.334¬µs"
I0110 21:48:19.419222       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="77.625¬µs"
I0110 21:53:23.540586       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="7.9865ms"
I0110 21:53:23.540823       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="111.542¬µs"
I0110 21:53:33.637378       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="11.3025ms"
I0110 21:53:33.637530       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="56.916¬µs"
I0110 21:53:47.410995       1 replica_set.go:676] "Finished syncing" kind="ReplicaSet" key="default/authentification-deployment-5484bdc86c" duration="104.958¬µs"

* 
* ==> kube-proxy [df25cf8ca18f] <==
* I0110 13:01:34.160044       1 server_others.go:69] "Using iptables proxy"
I0110 13:01:34.164783       1 node.go:141] Successfully retrieved node IP: 192.168.49.2
I0110 13:01:34.189618       1 server.go:632] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0110 13:01:34.190757       1 server_others.go:152] "Using iptables Proxier"
I0110 13:01:34.190775       1 server_others.go:421] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0110 13:01:34.190779       1 server_others.go:438] "Defaulting to no-op detect-local"
I0110 13:01:34.190885       1 proxier.go:251] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0110 13:01:34.191108       1 server.go:846] "Version info" version="v1.28.3"
I0110 13:01:34.191122       1 server.go:848] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0110 13:01:34.191469       1 config.go:188] "Starting service config controller"
I0110 13:01:34.191482       1 config.go:315] "Starting node config controller"
I0110 13:01:34.191494       1 shared_informer.go:311] Waiting for caches to sync for service config
I0110 13:01:34.191495       1 shared_informer.go:311] Waiting for caches to sync for node config
I0110 13:01:34.191634       1 config.go:97] "Starting endpoint slice config controller"
I0110 13:01:34.191646       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0110 13:01:34.292611       1 shared_informer.go:318] Caches are synced for node config
I0110 13:01:34.292634       1 shared_informer.go:318] Caches are synced for service config
I0110 13:01:34.292662       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [0d315ed67906] <==
* I0110 13:01:16.677299       1 serving.go:348] Generated self-signed cert in-memory
W0110 13:01:17.869832       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0110 13:01:17.869847       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0110 13:01:17.869852       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0110 13:01:17.869855       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0110 13:01:17.874647       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.28.3"
I0110 13:01:17.874657       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0110 13:01:17.875196       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0110 13:01:17.875211       1 shared_informer.go:311] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0110 13:01:17.875495       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0110 13:01:17.875508       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
W0110 13:01:17.875929       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0110 13:01:17.875947       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0110 13:01:17.876572       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0110 13:01:17.876587       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0110 13:01:17.876575       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0110 13:01:17.876598       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0110 13:01:17.876599       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0110 13:01:17.876606       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0110 13:01:17.876620       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0110 13:01:17.876614       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0110 13:01:17.876634       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0110 13:01:17.876624       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0110 13:01:17.876754       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0110 13:01:17.876763       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0110 13:01:17.876781       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0110 13:01:17.876784       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0110 13:01:17.876792       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0110 13:01:17.876794       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0110 13:01:17.876783       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0110 13:01:17.876805       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0110 13:01:17.876798       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0110 13:01:17.876811       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0110 13:01:17.876815       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0110 13:01:17.876819       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0110 13:01:17.876833       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0110 13:01:17.876839       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0110 13:01:17.876841       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0110 13:01:17.876844       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
W0110 13:01:17.876868       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0110 13:01:17.876880       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W0110 13:01:18.724345       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0110 13:01:18.724400       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
W0110 13:01:18.765639       1 reflector.go:535] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0110 13:01:18.765740       1 reflector.go:147] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
W0110 13:01:18.787892       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0110 13:01:18.787952       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0110 13:01:18.787905       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0110 13:01:18.788055       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0110 13:01:18.852561       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0110 13:01:18.852584       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0110 13:01:18.896789       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0110 13:01:18.896809       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0110 13:01:18.904401       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0110 13:01:18.904417       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0110 13:01:18.905861       1 reflector.go:535] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0110 13:01:18.905872       1 reflector.go:147] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
I0110 13:01:21.375705       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* Jan 10 19:32:25 minikube kubelet[2368]: W0110 19:32:25.425150    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 19:32:25 minikube kubelet[2368]: W0110 19:32:25.426779    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 19:37:25 minikube kubelet[2368]: W0110 19:37:25.424111    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 19:37:25 minikube kubelet[2368]: W0110 19:37:25.426294    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 19:42:25 minikube kubelet[2368]: W0110 19:42:25.420368    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 19:42:25 minikube kubelet[2368]: W0110 19:42:25.422110    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 19:47:25 minikube kubelet[2368]: W0110 19:47:25.445032    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 19:47:25 minikube kubelet[2368]: W0110 19:47:25.449459    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 19:52:25 minikube kubelet[2368]: W0110 19:52:25.445450    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 19:52:25 minikube kubelet[2368]: W0110 19:52:25.447703    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 19:57:25 minikube kubelet[2368]: W0110 19:57:25.443260    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 19:57:25 minikube kubelet[2368]: W0110 19:57:25.444138    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 20:02:25 minikube kubelet[2368]: W0110 20:02:25.445944    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 20:02:25 minikube kubelet[2368]: W0110 20:02:25.447513    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 20:07:25 minikube kubelet[2368]: W0110 20:07:25.445523    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 20:07:25 minikube kubelet[2368]: W0110 20:07:25.446472    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 20:12:25 minikube kubelet[2368]: W0110 20:12:25.404660    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 20:12:25 minikube kubelet[2368]: W0110 20:12:25.405471    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 20:17:25 minikube kubelet[2368]: W0110 20:17:25.404658    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 20:17:25 minikube kubelet[2368]: W0110 20:17:25.406508    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 20:22:25 minikube kubelet[2368]: W0110 20:22:25.403387    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 20:22:25 minikube kubelet[2368]: W0110 20:22:25.404937    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 20:27:25 minikube kubelet[2368]: W0110 20:27:25.404001    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 20:27:25 minikube kubelet[2368]: W0110 20:27:25.405563    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 20:32:25 minikube kubelet[2368]: W0110 20:32:25.406996    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 20:32:25 minikube kubelet[2368]: W0110 20:32:25.411237    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 20:37:25 minikube kubelet[2368]: W0110 20:37:25.405567    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 20:37:25 minikube kubelet[2368]: W0110 20:37:25.407845    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 20:42:25 minikube kubelet[2368]: W0110 20:42:25.400455    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 20:42:25 minikube kubelet[2368]: W0110 20:42:25.402544    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 20:47:25 minikube kubelet[2368]: W0110 20:47:25.396529    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 20:47:25 minikube kubelet[2368]: W0110 20:47:25.397646    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 20:52:25 minikube kubelet[2368]: W0110 20:52:25.405730    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 20:52:25 minikube kubelet[2368]: W0110 20:52:25.407701    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 20:57:25 minikube kubelet[2368]: W0110 20:57:25.413716    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 20:57:25 minikube kubelet[2368]: W0110 20:57:25.415718    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 21:02:25 minikube kubelet[2368]: W0110 21:02:25.413602    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 21:02:25 minikube kubelet[2368]: W0110 21:02:25.416078    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 21:07:25 minikube kubelet[2368]: W0110 21:07:25.412032    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 21:07:25 minikube kubelet[2368]: W0110 21:07:25.413743    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 21:12:25 minikube kubelet[2368]: W0110 21:12:25.413641    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 21:12:25 minikube kubelet[2368]: W0110 21:12:25.416201    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 21:17:25 minikube kubelet[2368]: W0110 21:17:25.357953    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 21:17:25 minikube kubelet[2368]: W0110 21:17:25.360896    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 21:22:25 minikube kubelet[2368]: W0110 21:22:25.356019    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 21:22:25 minikube kubelet[2368]: W0110 21:22:25.357583    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 21:27:25 minikube kubelet[2368]: W0110 21:27:25.350624    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 21:27:25 minikube kubelet[2368]: W0110 21:27:25.352527    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 21:32:25 minikube kubelet[2368]: W0110 21:32:25.350401    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 21:32:25 minikube kubelet[2368]: W0110 21:32:25.351983    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 21:37:25 minikube kubelet[2368]: W0110 21:37:25.409731    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 21:37:25 minikube kubelet[2368]: W0110 21:37:25.411353    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 21:42:25 minikube kubelet[2368]: W0110 21:42:25.409510    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 21:42:25 minikube kubelet[2368]: W0110 21:42:25.411494    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 21:47:25 minikube kubelet[2368]: W0110 21:47:25.412709    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 21:47:25 minikube kubelet[2368]: W0110 21:47:25.414579    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 21:52:25 minikube kubelet[2368]: W0110 21:52:25.412604    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 21:52:25 minikube kubelet[2368]: W0110 21:52:25.414232    2368 machine.go:65] Cannot read vendor id correctly, set empty.
Jan 10 21:57:25 minikube kubelet[2368]: W0110 21:57:25.417185    2368 sysinfo.go:203] Nodes topology is not available, providing CPU topology
Jan 10 21:57:25 minikube kubelet[2368]: W0110 21:57:25.418822    2368 machine.go:65] Cannot read vendor id correctly, set empty.

